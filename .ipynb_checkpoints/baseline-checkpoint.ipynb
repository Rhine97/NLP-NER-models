{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 说明\n",
    "从文本内容分解开始的baseline数据\n",
    "\n",
    "环境：（lenovo）base  \n",
    "python = 3.7.6  \n",
    "pytorch = 1.4.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import jieba\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchcrf import CRF    #This class provides an implementation of a CRF layer.\n",
    "from torch.utils.data import Dataset,DataLoader, RandomSampler\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各参数\n",
    "trainPath = \"./dataset./train_data_public.csv\"\n",
    "dicPath = \"./dataset./vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>BIO_anno</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...</td>\n",
       "      <td>B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>单标我有了，最近visa双标返现活动好</td>\n",
       "      <td>B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>建设银行提额很慢的……</td>\n",
       "      <td>B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k</td>\n",
       "      <td>O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>利率不错，可以撸</td>\n",
       "      <td>B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COM...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>7523</td>\n",
       "      <td>我鼎级拒了</td>\n",
       "      <td>O O O B-COMMENTS_ADJ O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>7524</td>\n",
       "      <td>一打一个准，准胜，看激活信用卡时那协议，全是对银行有利的</td>\n",
       "      <td>O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>7525</td>\n",
       "      <td>招行分期白80k</td>\n",
       "      <td>B-BANK I-BANK B-PRODUCT I-PRODUCT I-PRODUCT O O O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>7526</td>\n",
       "      <td>5万，额度还行吧没毕业哦</td>\n",
       "      <td>O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>7527</td>\n",
       "      <td>张家港农商、江阴农商、无锡农商试试</td>\n",
       "      <td>B-BANK I-BANK I-BANK I-BANK I-BANK O B-BANK I-...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7528 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0        0  交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...   \n",
       "1        1                                单标我有了，最近visa双标返现活动好   \n",
       "2        2                                        建设银行提额很慢的……   \n",
       "3        3                 我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k   \n",
       "4        4                                           利率不错，可以撸   \n",
       "...    ...                                                ...   \n",
       "7523  7523                                              我鼎级拒了   \n",
       "7524  7524                       一打一个准，准胜，看激活信用卡时那协议，全是对银行有利的   \n",
       "7525  7525                                           招行分期白80k   \n",
       "7526  7526                                       5万，额度还行吧没毕业哦   \n",
       "7527  7527                                  张家港农商、江阴农商、无锡农商试试   \n",
       "\n",
       "                                               BIO_anno  class  \n",
       "0     B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...      0  \n",
       "1     B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...      1  \n",
       "2     B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...      0  \n",
       "3     O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...      2  \n",
       "4     B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COM...      1  \n",
       "...                                                 ...    ...  \n",
       "7523                             O O O B-COMMENTS_ADJ O      2  \n",
       "7524  O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...      2  \n",
       "7525  B-BANK I-BANK B-PRODUCT I-PRODUCT I-PRODUCT O O O      2  \n",
       "7526      O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O      2  \n",
       "7527  B-BANK I-BANK I-BANK I-BANK I-BANK O B-BANK I-...      2  \n",
       "\n",
       "[7528 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile(trainPath):\n",
    "    print(\"file exist\")\n",
    "    dataset = pd.read_csv(trainPath)\n",
    "\n",
    "dataset   #共10000条数据，属性分为：unnamed，text，BIO_anno，class，bank_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...\n",
       "1                                  单标我有了，最近visa双标返现活动好\n",
       "2                                          建设银行提额很慢的……\n",
       "3                   我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k\n",
       "4                                             利率不错，可以撸\n",
       "5                             不能??好像房贷跟信用卡是分开审核的反正我的不得\n",
       "6    我感觉这样才是合理的，花呗白条没要那么多信息，照样可以给额度。有征信威慑，没那么多人敢借了不...\n",
       "7    羡慕，可能上个月申请多了。上月连续下了浦发广发华夏交通。这个月申请建行，农业??邮储各种秒拒...\n",
       "8    这个短债只是用来提升信誉刷建行预审批的，又不是什么赚钱的基金。也只有建行有卖，我买过，不但不...\n",
       "9                                       打电话问中信信用卡为啥没批呗\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7528"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dataset['text']\n",
    "text[:10]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7528, 50])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7528, 50])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.shape\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch_data in enumerate(train_loader):\n",
    "    _text = batch_data['texts']\n",
    "    _label = batch_data['labels']\n",
    "    if step>=0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_text.shape\n",
    "_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2,  ..., 0, 0, 0],\n",
       "        [0, 3, 4,  ..., 0, 0, 0],\n",
       "        [0, 1, 2,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 2,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 4, 4, 3],\n",
       "        [4, 5, 6,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 32])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_label\n",
    "_label.shape\n",
    "_label.view(MAX_LEN,len(_label))\n",
    "_label.view(MAX_LEN,len(_label)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9629, 0.1097, 0.6713, 0.6168, 0.7762],\n",
       "         [0.6932, 0.8817, 0.1901, 0.5406, 0.3526]],\n",
       "\n",
       "        [[0.0210, 0.7553, 0.6139, 0.6274, 0.7161],\n",
       "         [0.9437, 0.4907, 0.2220, 0.7273, 0.2480]],\n",
       "\n",
       "        [[0.2442, 0.7787, 0.8270, 0.7818, 0.0614],\n",
       "         [0.1880, 0.1647, 0.3665, 0.5828, 0.2644]],\n",
       "\n",
       "        [[0.7119, 0.1657, 0.5518, 0.6041, 0.0893],\n",
       "         [0.2549, 0.0019, 0.6049, 0.5491, 0.1012]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9629, 0.1097, 0.6713, 0.6168, 0.7762],\n",
       "         [0.6932, 0.8817, 0.1901, 0.5406, 0.3526],\n",
       "         [0.0210, 0.7553, 0.6139, 0.6274, 0.7161],\n",
       "         [0.9437, 0.4907, 0.2220, 0.7273, 0.2480]],\n",
       "\n",
       "        [[0.2442, 0.7787, 0.8270, 0.7818, 0.0614],\n",
       "         [0.1880, 0.1647, 0.3665, 0.5828, 0.2644],\n",
       "         [0.7119, 0.1657, 0.5518, 0.6041, 0.0893],\n",
       "         [0.2549, 0.0019, 0.6049, 0.5491, 0.1012]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9629, 0.1097, 0.6713, 0.6168, 0.7762],\n",
       "         [0.0210, 0.7553, 0.6139, 0.6274, 0.7161],\n",
       "         [0.2442, 0.7787, 0.8270, 0.7818, 0.0614],\n",
       "         [0.7119, 0.1657, 0.5518, 0.6041, 0.0893]],\n",
       "\n",
       "        [[0.6932, 0.8817, 0.1901, 0.5406, 0.3526],\n",
       "         [0.9437, 0.4907, 0.2220, 0.7273, 0.2480],\n",
       "         [0.1880, 0.1647, 0.3665, 0.5828, 0.2644],\n",
       "         [0.2549, 0.0019, 0.6049, 0.5491, 0.1012]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(4, 2, 5)\n",
    "a\n",
    "a.reshape(2,4,5)\n",
    "\n",
    "a.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>BIO_anno</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...</td>\n",
       "      <td>B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>单标我有了，最近visa双标返现活动好</td>\n",
       "      <td>B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>建设银行提额很慢的……</td>\n",
       "      <td>B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k</td>\n",
       "      <td>O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>利率不错，可以撸</td>\n",
       "      <td>B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COM...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>7523</td>\n",
       "      <td>我鼎级拒了</td>\n",
       "      <td>O O O B-COMMENTS_ADJ O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>7524</td>\n",
       "      <td>一打一个准，准胜，看激活信用卡时那协议，全是对银行有利的</td>\n",
       "      <td>O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>7525</td>\n",
       "      <td>招行分期白80k</td>\n",
       "      <td>B-BANK I-BANK B-PRODUCT I-PRODUCT I-PRODUCT O O O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>7526</td>\n",
       "      <td>5万，额度还行吧没毕业哦</td>\n",
       "      <td>O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>7527</td>\n",
       "      <td>张家港农商、江阴农商、无锡农商试试</td>\n",
       "      <td>B-BANK I-BANK I-BANK I-BANK I-BANK O B-BANK I-...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7528 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0        0  交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...   \n",
       "1        1                                单标我有了，最近visa双标返现活动好   \n",
       "2        2                                        建设银行提额很慢的……   \n",
       "3        3                 我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k   \n",
       "4        4                                           利率不错，可以撸   \n",
       "...    ...                                                ...   \n",
       "7523  7523                                              我鼎级拒了   \n",
       "7524  7524                       一打一个准，准胜，看激活信用卡时那协议，全是对银行有利的   \n",
       "7525  7525                                           招行分期白80k   \n",
       "7526  7526                                       5万，额度还行吧没毕业哦   \n",
       "7527  7527                                  张家港农商、江阴农商、无锡农商试试   \n",
       "\n",
       "                                               BIO_anno  class  \n",
       "0     B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...      0  \n",
       "1     B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...      1  \n",
       "2     B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...      0  \n",
       "3     O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...      2  \n",
       "4     B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COM...      1  \n",
       "...                                                 ...    ...  \n",
       "7523                             O O O B-COMMENTS_ADJ O      2  \n",
       "7524  O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...      2  \n",
       "7525  B-BANK I-BANK B-PRODUCT I-PRODUCT I-PRODUCT O O O      2  \n",
       "7526      O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O      2  \n",
       "7527  B-BANK I-BANK I-BANK I-BANK I-BANK O B-BANK I-...      2  \n",
       "\n",
       "[7528 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获取 tag to index 词典\n",
    "def get_tag2index():\n",
    "    return {\"O\": 0,\n",
    "            \"B-BANK\":1,\"I-BANK\":2,         #银行实体\n",
    "            \"B-PRODUCT\":3,\"I-PRODUCT\":4,   #产品实体\n",
    "            \"B-COMMENTS_N\":5,\"I-COMMENTS_N\":6,   #用户评论，名词\n",
    "            \"B-COMMENTS_ADJ\":7,\"I-COMMENTS_ADJ\":8    #用户评论，形容词\n",
    "            }\n",
    "# 获取 word to index 词典\n",
    "def get_w2i(vocab_path = dicPath):\n",
    "    w2i = {}\n",
    "    with open(vocab_path, encoding = 'utf-8') as f:\n",
    "        while True:\n",
    "            text = f.readline()\n",
    "            if not text:\n",
    "                break\n",
    "            text = text.strip()\n",
    "            if text and len(text) > 0:\n",
    "                w2i[text] = len(w2i) + 1\n",
    "    return w2i\n",
    "\n",
    "\n",
    "unk_flag = '[UNK]'\n",
    "pad_flag = '[PAD]'\n",
    "start_flag = '[STA]'\n",
    "end_flag = '[END]' \n",
    "\n",
    "w2i = get_w2i()   #获得tag_to_index词典\n",
    "tag2i = get_tag2index()\n",
    "\n",
    "#w2i\n",
    "unk_index = w2i.get(unk_flag, 101)\n",
    "pad_index = w2i.get(pad_flag, 1)\n",
    "start_index = w2i.get(start_flag, 102)    #开始\n",
    "end_index = w2i.get(end_flag, 103)   #中间截至（主要用在有上下句的情况下）\n",
    "\n",
    "MAX_LEN = 50    #句子的标准长度\n",
    "BATCH_SIZE = 32  #minibatch的大小\n",
    "EMBEDDING_DIM = 120\n",
    "HIDDEN_DIM = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tag_to_index(dataset):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for row in range(len(dataset)):\n",
    "        text = dataset.iloc[row]['text']\n",
    "        tag = dataset.iloc[row]['BIO_anno']\n",
    "        text\n",
    "        tag\n",
    "        if len(text)!=len(tag):\n",
    "            next\n",
    "        #print(\"text长度：\"+str(len(text)))\n",
    "\n",
    "        #1. word转index\n",
    "        #1.1 text词汇\n",
    "        text_index = []\n",
    "        text_index.append(start_index)   #先加入开头index\n",
    "        for word in text:\n",
    "            text_index.append(w2i.get(word, unk_index))   #将当前词转成词典对应index，或不认识标注UNK的index\n",
    "        text_index.append(end_index)   #最后加个结尾index\n",
    "        #index\n",
    "        #1.2 tag标签\n",
    "        tag = tag.split()\n",
    "        tag_index = [tag2i.get(t,0) for t in tag]\n",
    "        tag_index = [0] + tag_index + [0]\n",
    "\n",
    "        #2. 填充或截至句子至标准长度\n",
    "        #2.1 text词汇&tag标签\n",
    "        if len(text_index)<MAX_LEN:    #句子短，补充pad_index到满够MAX_LEN\n",
    "            pad_len = MAX_LEN-len(text_index)\n",
    "            text_index = text_index + [pad_index]*pad_len\n",
    "            tag_index = tag_index + [0]*pad_len\n",
    "        elif len(text_index)>MAX_LEN:  #句子过长，截断\n",
    "            text_index = text_index[:MAX_LEN-1]\n",
    "            text_index.append(end_index)\n",
    "            tag_index = tag_index[:MAX_LEN-1]\n",
    "            tag_index.append(0)\n",
    "        texts.append(text_index)\n",
    "        labels.append(tag_index)\n",
    "        \n",
    "    #把list类型的转成tensor类型，方便后期进行训练\n",
    "    texts = torch.LongTensor(texts)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    #texts = torch.as_tensor(torch.from_numpy(np.array(texts)), dtype=torch.int32)\n",
    "    #labels = torch.as_tensor(torch.from_numpy(np.array(labels)), dtype=torch.int32)\n",
    "    return texts,labels\n",
    "\n",
    "class MiDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.dataset =  texts #torch.tensor(texts)\n",
    "        self.labels = labels    #torch.tensor(labels)\n",
    "        \n",
    "        self.nums = len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = {'texts': self.dataset[index],\n",
    "                    'labels': self.labels[index]}\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7528, 50])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7528, 50])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts,labels = text_tag_to_index(dataset)\n",
    "texts.shape\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MiDataset at 0x27595b33848>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MiDataset(texts,labels)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2759545c2c8>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_sampler = RandomSampler(train_dataset)    #将训练集打乱\n",
    "train_loader = DataLoader(dataset=train_dataset,   #按batch_size加载训练集\n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          #sampler=train_sampler,\n",
    "                                          num_workers=0,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=False)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:1\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:2\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:3\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:4\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:5\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:6\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:7\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:8\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:9\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:10\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:11\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:12\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:13\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:14\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:15\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:16\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:17\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:18\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:19\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:20\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:21\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:22\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:23\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:24\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:25\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:26\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:27\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:28\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:29\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:30\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:31\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:32\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:33\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:34\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:35\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:36\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:37\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:38\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:39\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:40\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:41\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:42\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:43\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:44\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:45\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:46\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:47\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:48\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:49\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:50\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:51\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:52\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:53\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:54\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:55\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:56\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:57\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:58\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:59\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:60\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:61\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:62\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:63\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:64\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:65\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:66\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:67\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:68\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:69\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:70\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:71\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:72\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:73\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:74\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:75\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:76\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:77\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:78\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:79\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:80\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:81\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:82\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:83\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:84\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:85\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:86\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:87\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:88\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:89\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:90\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:91\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:92\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:93\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:94\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:95\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:96\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:97\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:98\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:99\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:100\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:101\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:102\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:103\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:104\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:105\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:106\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:107\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:108\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:109\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:110\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:111\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:112\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:113\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:114\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:115\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:116\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:117\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:118\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:119\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:120\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:121\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:122\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:123\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:124\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:125\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:126\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:127\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:128\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:129\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:130\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:131\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:132\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:133\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:134\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:135\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:136\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:137\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:138\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:139\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:140\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:141\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:142\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:143\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:144\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:145\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:146\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:147\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:148\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:149\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:150\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:151\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:152\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:153\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:154\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:155\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:156\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:157\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:158\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:159\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:160\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:161\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:162\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:163\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:164\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:165\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:166\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:167\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:168\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:169\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:170\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:171\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:172\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:173\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:174\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:175\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:176\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:177\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:178\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:179\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:180\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:181\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:182\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:183\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:184\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:185\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:186\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:187\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:188\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:189\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:190\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:191\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:192\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:193\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:194\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:195\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:196\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:197\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:198\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:199\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:200\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:201\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:202\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:203\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:204\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:205\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:206\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:207\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:208\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:209\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:210\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:211\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:212\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:213\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:214\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:215\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:216\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:217\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:218\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:219\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:220\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:221\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:222\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:223\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:224\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:225\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:226\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:227\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:228\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:229\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:230\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:231\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:232\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:233\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:234\n",
      "texts:torch.Size([32, 50])\n",
      "labels:torch.Size([32, 50])\n",
      "step:235\n",
      "texts:torch.Size([8, 50])\n",
      "labels:torch.Size([8, 50])\n"
     ]
    }
   ],
   "source": [
    "#把第一个batch_data拿出来看看\n",
    "for step, batch_data in enumerate(train_loader):\n",
    "    print(\"step:\"+str(step))\n",
    "    print(\"texts:\"+str(batch_data['texts'].shape))\n",
    "    #batch_data['dataset']\n",
    "    print(\"labels:\"+str(batch_data['labels'].shape))\n",
    "    #batch_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        \n",
    "        #####中间层设置\n",
    "        #embedding层\n",
    "        self.word_embeds = nn.Embedding(vocab_size,embedding_dim)  #转词向量\n",
    "        #lstm层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, num_layers = 1, bidirectional = True)\n",
    "        #LSTM的输出对应tag空间（tag space）\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)  #输入是[batch_size, size]中的size，输出是[batch_size，output_size]的output_size\n",
    "        #CRF层\n",
    "        self.crf = CRF(self.tagset_size)   #默认batch_first=False\n",
    "        \n",
    "        #transition参数矩阵，entry i,j是transitioning从j到i的评分\n",
    "        #这是什么？\n",
    "        #self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        #强制约束：不会transfer到开始tag，也不会从停止tag开始transfer\n",
    "        #self.transitions.data[tag_to_ix[START_TAG],:]\n",
    "        #self.transitions.data[:, tag_to_ix[STOP_TAG]]\n",
    "        \n",
    "        #lstm层的隐藏节点设置\n",
    "        self.hidden = (torch.randn(2,1,self.hidden_dim//2),torch.randn(2,1,self.hidden_dim//2))#self.init_hidden()\n",
    "    \n",
    "   \n",
    "    def forward(self, sentence, tags):     #sentence=(batch,seq_len)   tags=(batch,seq_len)\n",
    "        #1. 从sentence到Embedding层\n",
    "        embeds = self.word_embeds(sentence).permute(1,0,2)#.view(MAX_LEN,len(sentence),-1)   #output=[seq_len, batch_size, embedding_size]\n",
    "        \n",
    "        #2. 从Embedding层到BiLSTM层\n",
    "        #隐藏层就是（h_0,c_0）   h_0的结构：(num_layers*num_directions,batch_size,hidden_size)=(2, 1, hidden_size=4//2=2)\n",
    "        self.hidden = (torch.randn(2,len(sentence),self.hidden_dim//2),torch.randn(2,len(sentence),self.hidden_dim//2))  #修改进来 shape=((2,1,2),(2,1,2))  \n",
    "        #input=(seq_length,batch_size,input_size)的张量  \n",
    "        #output=(seq_length,batch_size,num_directions*hidden_size)=(MAX_LEN, len(sentence), 2)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden) \n",
    "        \n",
    "        #3. 从BiLSTM层到全连接层\n",
    "        #从lstm的输出转为tagset_size长度的向量组（即输出了每个tag的可能性）\n",
    "        #输出shape=(MAX_LEN, batch_size, len(tag_to_ix))\n",
    "        lstm_feats = self.hidden2tag(lstm_out)    \n",
    "        \n",
    "        #4. 全连接层到CRF层\n",
    "        #乘以-1是因为输出的log值嘛？loss越靠近0的话，值越是负数的较大值（我靠，有道理）\n",
    "        outputs = -1.*self.crf(emissions=lstm_feats,tags=tags.permute(1,0),reduction='mean')   #outputs=(batch_size,)   输出log形式的likelihood\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(w2i), tag2i, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(21127, 120)\n",
       "  (lstm): LSTM(120, 6, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=12, out_features=9, bias=True)\n",
       "  (crf): CRF(num_tags=9)\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch---------------0\n",
      "loss-----tensor(11.4074, grad_fn=<MulBackward0>)\n",
      "epoch---------------1\n",
      "loss-----tensor(8.9804, grad_fn=<MulBackward0>)\n",
      "epoch---------------2\n",
      "loss-----tensor(7.8388, grad_fn=<MulBackward0>)\n",
      "epoch---------------3\n",
      "loss-----tensor(7.1823, grad_fn=<MulBackward0>)\n",
      "epoch---------------4\n",
      "loss-----tensor(6.6514, grad_fn=<MulBackward0>)\n",
      "epoch---------------5\n",
      "loss-----tensor(6.2408, grad_fn=<MulBackward0>)\n",
      "epoch---------------6\n",
      "loss-----tensor(5.9806, grad_fn=<MulBackward0>)\n",
      "epoch---------------7\n",
      "loss-----tensor(5.6832, grad_fn=<MulBackward0>)\n",
      "epoch---------------8\n",
      "loss-----tensor(5.2639, grad_fn=<MulBackward0>)\n",
      "epoch---------------9\n",
      "loss-----tensor(4.9739, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(\"epoch---------------\"+str(epoch))\n",
    "    for step, batch_data in enumerate(train_loader):\n",
    "        # 1. 清空梯度\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 2. 运行模型\n",
    "        loss = model(batch_data['texts'], batch_data['labels']) \n",
    "        \n",
    "        # 3. 计算loss值，梯度并更新权重参数                                 \n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "    print(\"loss-----\"+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17.1218)\n"
     ]
    }
   ],
   "source": [
    "# Check predictions before training\n",
    "with torch.no_grad():   #这部分的代码不用跟踪反向梯度更新\n",
    "    precheck_sent = torch.tensor([word_to_ix[t] for t in training_data[0][0]], dtype=torch.long)   #第一句每个词的index值\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    #print(model._get_lstm_features(precheck_sent))   #shape=(len(sentence)=11, 1, 5)\n",
    "    print(model(precheck_sent,precheck_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "loss-----tensor(10.4088, grad_fn=<MulBackward0>)\n",
      "epoch: 1\n",
      "loss-----tensor(10.0381, grad_fn=<MulBackward0>)\n",
      "epoch: 2\n",
      "loss-----tensor(9.6561, grad_fn=<MulBackward0>)\n",
      "epoch: 3\n",
      "loss-----tensor(9.3728, grad_fn=<MulBackward0>)\n",
      "epoch: 4\n",
      "loss-----tensor(9.0329, grad_fn=<MulBackward0>)\n",
      "epoch: 5\n",
      "loss-----tensor(8.8586, grad_fn=<MulBackward0>)\n",
      "epoch: 6\n",
      "loss-----tensor(8.4515, grad_fn=<MulBackward0>)\n",
      "epoch: 7\n",
      "loss-----tensor(8.2664, grad_fn=<MulBackward0>)\n",
      "epoch: 8\n",
      "loss-----tensor(8.0377, grad_fn=<MulBackward0>)\n",
      "epoch: 9\n",
      "loss-----tensor(7.8890, grad_fn=<MulBackward0>)\n",
      "epoch: 10\n",
      "loss-----tensor(7.6858, grad_fn=<MulBackward0>)\n",
      "epoch: 11\n",
      "loss-----tensor(7.5024, grad_fn=<MulBackward0>)\n",
      "epoch: 12\n",
      "loss-----tensor(7.3859, grad_fn=<MulBackward0>)\n",
      "epoch: 13\n",
      "loss-----tensor(7.2079, grad_fn=<MulBackward0>)\n",
      "epoch: 14\n",
      "loss-----tensor(7.1478, grad_fn=<MulBackward0>)\n",
      "epoch: 15\n",
      "loss-----tensor(7.0036, grad_fn=<MulBackward0>)\n",
      "epoch: 16\n",
      "loss-----tensor(6.8604, grad_fn=<MulBackward0>)\n",
      "epoch: 17\n",
      "loss-----tensor(6.7793, grad_fn=<MulBackward0>)\n",
      "epoch: 18\n",
      "loss-----tensor(6.6007, grad_fn=<MulBackward0>)\n",
      "epoch: 19\n",
      "loss-----tensor(6.5087, grad_fn=<MulBackward0>)\n",
      "epoch: 20\n",
      "loss-----tensor(6.4170, grad_fn=<MulBackward0>)\n",
      "epoch: 21\n",
      "loss-----tensor(6.3193, grad_fn=<MulBackward0>)\n",
      "epoch: 22\n",
      "loss-----tensor(6.2351, grad_fn=<MulBackward0>)\n",
      "epoch: 23\n",
      "loss-----tensor(6.2125, grad_fn=<MulBackward0>)\n",
      "epoch: 24\n",
      "loss-----tensor(6.0675, grad_fn=<MulBackward0>)\n",
      "epoch: 25\n",
      "loss-----tensor(5.9085, grad_fn=<MulBackward0>)\n",
      "epoch: 26\n",
      "loss-----tensor(5.8824, grad_fn=<MulBackward0>)\n",
      "epoch: 27\n",
      "loss-----tensor(5.8950, grad_fn=<MulBackward0>)\n",
      "epoch: 28\n",
      "loss-----tensor(5.7521, grad_fn=<MulBackward0>)\n",
      "epoch: 29\n",
      "loss-----tensor(5.6945, grad_fn=<MulBackward0>)\n",
      "epoch: 30\n",
      "loss-----tensor(5.6111, grad_fn=<MulBackward0>)\n",
      "epoch: 31\n",
      "loss-----tensor(5.5031, grad_fn=<MulBackward0>)\n",
      "epoch: 32\n",
      "loss-----tensor(5.5670, grad_fn=<MulBackward0>)\n",
      "epoch: 33\n",
      "loss-----tensor(5.4439, grad_fn=<MulBackward0>)\n",
      "epoch: 34\n",
      "loss-----tensor(5.4049, grad_fn=<MulBackward0>)\n",
      "epoch: 35\n",
      "loss-----tensor(5.3642, grad_fn=<MulBackward0>)\n",
      "epoch: 36\n",
      "loss-----tensor(5.1601, grad_fn=<MulBackward0>)\n",
      "epoch: 37\n",
      "loss-----tensor(5.1890, grad_fn=<MulBackward0>)\n",
      "epoch: 38\n",
      "loss-----tensor(5.1542, grad_fn=<MulBackward0>)\n",
      "epoch: 39\n",
      "loss-----tensor(5.0494, grad_fn=<MulBackward0>)\n",
      "epoch: 40\n",
      "loss-----tensor(5.0044, grad_fn=<MulBackward0>)\n",
      "epoch: 41\n",
      "loss-----tensor(4.9315, grad_fn=<MulBackward0>)\n",
      "epoch: 42\n",
      "loss-----tensor(4.8711, grad_fn=<MulBackward0>)\n",
      "epoch: 43\n",
      "loss-----tensor(4.8083, grad_fn=<MulBackward0>)\n",
      "epoch: 44\n",
      "loss-----tensor(4.7643, grad_fn=<MulBackward0>)\n",
      "epoch: 45\n",
      "loss-----tensor(4.7158, grad_fn=<MulBackward0>)\n",
      "epoch: 46\n",
      "loss-----tensor(4.6733, grad_fn=<MulBackward0>)\n",
      "epoch: 47\n",
      "loss-----tensor(4.8273, grad_fn=<MulBackward0>)\n",
      "epoch: 48\n",
      "loss-----tensor(4.5944, grad_fn=<MulBackward0>)\n",
      "epoch: 49\n",
      "loss-----tensor(4.6271, grad_fn=<MulBackward0>)\n",
      "epoch: 50\n",
      "loss-----tensor(4.4898, grad_fn=<MulBackward0>)\n",
      "epoch: 51\n",
      "loss-----tensor(4.4482, grad_fn=<MulBackward0>)\n",
      "epoch: 52\n",
      "loss-----tensor(4.5082, grad_fn=<MulBackward0>)\n",
      "epoch: 53\n",
      "loss-----tensor(4.2876, grad_fn=<MulBackward0>)\n",
      "epoch: 54\n",
      "loss-----tensor(4.3538, grad_fn=<MulBackward0>)\n",
      "epoch: 55\n",
      "loss-----tensor(4.3073, grad_fn=<MulBackward0>)\n",
      "epoch: 56\n",
      "loss-----tensor(4.2163, grad_fn=<MulBackward0>)\n",
      "epoch: 57\n",
      "loss-----tensor(4.2459, grad_fn=<MulBackward0>)\n",
      "epoch: 58\n",
      "loss-----tensor(4.1630, grad_fn=<MulBackward0>)\n",
      "epoch: 59\n",
      "loss-----tensor(4.1154, grad_fn=<MulBackward0>)\n",
      "epoch: 60\n",
      "loss-----tensor(4.2239, grad_fn=<MulBackward0>)\n",
      "epoch: 61\n",
      "loss-----tensor(4.0137, grad_fn=<MulBackward0>)\n",
      "epoch: 62\n",
      "loss-----tensor(4.1361, grad_fn=<MulBackward0>)\n",
      "epoch: 63\n",
      "loss-----tensor(3.9715, grad_fn=<MulBackward0>)\n",
      "epoch: 64\n",
      "loss-----tensor(3.9339, grad_fn=<MulBackward0>)\n",
      "epoch: 65\n",
      "loss-----tensor(4.0394, grad_fn=<MulBackward0>)\n",
      "epoch: 66\n",
      "loss-----tensor(3.8836, grad_fn=<MulBackward0>)\n",
      "epoch: 67\n",
      "loss-----tensor(3.8206, grad_fn=<MulBackward0>)\n",
      "epoch: 68\n",
      "loss-----tensor(3.8533, grad_fn=<MulBackward0>)\n",
      "epoch: 69\n",
      "loss-----tensor(3.7703, grad_fn=<MulBackward0>)\n",
      "epoch: 70\n",
      "loss-----tensor(3.7477, grad_fn=<MulBackward0>)\n",
      "epoch: 71\n",
      "loss-----tensor(3.6538, grad_fn=<MulBackward0>)\n",
      "epoch: 72\n",
      "loss-----tensor(3.6735, grad_fn=<MulBackward0>)\n",
      "epoch: 73\n",
      "loss-----tensor(3.6320, grad_fn=<MulBackward0>)\n",
      "epoch: 74\n",
      "loss-----tensor(3.6482, grad_fn=<MulBackward0>)\n",
      "epoch: 75\n",
      "loss-----tensor(3.5686, grad_fn=<MulBackward0>)\n",
      "epoch: 76\n",
      "loss-----tensor(3.5690, grad_fn=<MulBackward0>)\n",
      "epoch: 77\n",
      "loss-----tensor(3.6299, grad_fn=<MulBackward0>)\n",
      "epoch: 78\n",
      "loss-----tensor(3.4692, grad_fn=<MulBackward0>)\n",
      "epoch: 79\n",
      "loss-----tensor(3.4567, grad_fn=<MulBackward0>)\n",
      "epoch: 80\n",
      "loss-----tensor(3.4565, grad_fn=<MulBackward0>)\n",
      "epoch: 81\n",
      "loss-----tensor(3.5530, grad_fn=<MulBackward0>)\n",
      "epoch: 82\n",
      "loss-----tensor(3.4311, grad_fn=<MulBackward0>)\n",
      "epoch: 83\n",
      "loss-----tensor(3.3450, grad_fn=<MulBackward0>)\n",
      "epoch: 84\n",
      "loss-----tensor(3.3390, grad_fn=<MulBackward0>)\n",
      "epoch: 85\n",
      "loss-----tensor(3.3245, grad_fn=<MulBackward0>)\n",
      "epoch: 86\n",
      "loss-----tensor(3.2933, grad_fn=<MulBackward0>)\n",
      "epoch: 87\n",
      "loss-----tensor(3.2236, grad_fn=<MulBackward0>)\n",
      "epoch: 88\n",
      "loss-----tensor(3.3687, grad_fn=<MulBackward0>)\n",
      "epoch: 89\n",
      "loss-----tensor(3.2218, grad_fn=<MulBackward0>)\n",
      "epoch: 90\n",
      "loss-----tensor(3.2110, grad_fn=<MulBackward0>)\n",
      "epoch: 91\n",
      "loss-----tensor(3.2010, grad_fn=<MulBackward0>)\n",
      "epoch: 92\n",
      "loss-----tensor(3.3210, grad_fn=<MulBackward0>)\n",
      "epoch: 93\n",
      "loss-----tensor(3.1297, grad_fn=<MulBackward0>)\n",
      "epoch: 94\n",
      "loss-----tensor(3.0919, grad_fn=<MulBackward0>)\n",
      "epoch: 95\n",
      "loss-----tensor(3.0455, grad_fn=<MulBackward0>)\n",
      "epoch: 96\n",
      "loss-----tensor(3.2068, grad_fn=<MulBackward0>)\n",
      "epoch: 97\n",
      "loss-----tensor(2.9976, grad_fn=<MulBackward0>)\n",
      "epoch: 98\n",
      "loss-----tensor(2.9915, grad_fn=<MulBackward0>)\n",
      "epoch: 99\n",
      "loss-----tensor(3.0053, grad_fn=<MulBackward0>)\n",
      "epoch: 100\n",
      "loss-----tensor(2.9928, grad_fn=<MulBackward0>)\n",
      "epoch: 101\n",
      "loss-----tensor(2.9840, grad_fn=<MulBackward0>)\n",
      "epoch: 102\n",
      "loss-----tensor(2.8875, grad_fn=<MulBackward0>)\n",
      "epoch: 103\n",
      "loss-----tensor(2.8738, grad_fn=<MulBackward0>)\n",
      "epoch: 104\n",
      "loss-----tensor(2.9757, grad_fn=<MulBackward0>)\n",
      "epoch: 105\n",
      "loss-----tensor(2.8763, grad_fn=<MulBackward0>)\n",
      "epoch: 106\n",
      "loss-----tensor(2.9182, grad_fn=<MulBackward0>)\n",
      "epoch: 107\n",
      "loss-----tensor(2.8438, grad_fn=<MulBackward0>)\n",
      "epoch: 108\n",
      "loss-----tensor(2.7933, grad_fn=<MulBackward0>)\n",
      "epoch: 109\n",
      "loss-----tensor(2.7865, grad_fn=<MulBackward0>)\n",
      "epoch: 110\n",
      "loss-----tensor(2.7712, grad_fn=<MulBackward0>)\n",
      "epoch: 111\n",
      "loss-----tensor(2.7371, grad_fn=<MulBackward0>)\n",
      "epoch: 112\n",
      "loss-----tensor(2.6604, grad_fn=<MulBackward0>)\n",
      "epoch: 113\n",
      "loss-----tensor(2.6637, grad_fn=<MulBackward0>)\n",
      "epoch: 114\n",
      "loss-----tensor(2.6143, grad_fn=<MulBackward0>)\n",
      "epoch: 115\n",
      "loss-----tensor(2.6600, grad_fn=<MulBackward0>)\n",
      "epoch: 116\n",
      "loss-----tensor(2.6322, grad_fn=<MulBackward0>)\n",
      "epoch: 117\n",
      "loss-----tensor(2.6470, grad_fn=<MulBackward0>)\n",
      "epoch: 118\n",
      "loss-----tensor(2.5593, grad_fn=<MulBackward0>)\n",
      "epoch: 119\n",
      "loss-----tensor(2.6741, grad_fn=<MulBackward0>)\n",
      "epoch: 120\n",
      "loss-----tensor(2.5334, grad_fn=<MulBackward0>)\n",
      "epoch: 121\n",
      "loss-----tensor(2.6875, grad_fn=<MulBackward0>)\n",
      "epoch: 122\n",
      "loss-----tensor(2.5077, grad_fn=<MulBackward0>)\n",
      "epoch: 123\n",
      "loss-----tensor(2.4914, grad_fn=<MulBackward0>)\n",
      "epoch: 124\n",
      "loss-----tensor(2.4589, grad_fn=<MulBackward0>)\n",
      "epoch: 125\n",
      "loss-----tensor(2.5198, grad_fn=<MulBackward0>)\n",
      "epoch: 126\n",
      "loss-----tensor(2.4369, grad_fn=<MulBackward0>)\n",
      "epoch: 127\n",
      "loss-----tensor(2.4881, grad_fn=<MulBackward0>)\n",
      "epoch: 128\n",
      "loss-----tensor(2.4902, grad_fn=<MulBackward0>)\n",
      "epoch: 129\n",
      "loss-----tensor(2.3581, grad_fn=<MulBackward0>)\n",
      "epoch: 130\n",
      "loss-----tensor(2.3863, grad_fn=<MulBackward0>)\n",
      "epoch: 131\n",
      "loss-----tensor(2.3892, grad_fn=<MulBackward0>)\n",
      "epoch: 132\n",
      "loss-----tensor(2.3073, grad_fn=<MulBackward0>)\n",
      "epoch: 133\n",
      "loss-----tensor(2.2954, grad_fn=<MulBackward0>)\n",
      "epoch: 134\n",
      "loss-----tensor(2.2816, grad_fn=<MulBackward0>)\n",
      "epoch: 135\n",
      "loss-----tensor(2.2763, grad_fn=<MulBackward0>)\n",
      "epoch: 136\n",
      "loss-----tensor(2.2397, grad_fn=<MulBackward0>)\n",
      "epoch: 137\n",
      "loss-----tensor(2.2169, grad_fn=<MulBackward0>)\n",
      "epoch: 138\n",
      "loss-----tensor(2.1689, grad_fn=<MulBackward0>)\n",
      "epoch: 139\n",
      "loss-----tensor(2.2149, grad_fn=<MulBackward0>)\n",
      "epoch: 140\n",
      "loss-----tensor(2.1437, grad_fn=<MulBackward0>)\n",
      "epoch: 141\n",
      "loss-----tensor(2.1581, grad_fn=<MulBackward0>)\n",
      "epoch: 142\n",
      "loss-----tensor(2.1909, grad_fn=<MulBackward0>)\n",
      "epoch: 143\n",
      "loss-----tensor(2.1549, grad_fn=<MulBackward0>)\n",
      "epoch: 144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss-----tensor(2.1967, grad_fn=<MulBackward0>)\n",
      "epoch: 145\n",
      "loss-----tensor(2.0932, grad_fn=<MulBackward0>)\n",
      "epoch: 146\n",
      "loss-----tensor(2.0208, grad_fn=<MulBackward0>)\n",
      "epoch: 147\n",
      "loss-----tensor(2.1186, grad_fn=<MulBackward0>)\n",
      "epoch: 148\n",
      "loss-----tensor(2.0314, grad_fn=<MulBackward0>)\n",
      "epoch: 149\n",
      "loss-----tensor(2.0632, grad_fn=<MulBackward0>)\n",
      "epoch: 150\n",
      "loss-----tensor(1.9543, grad_fn=<MulBackward0>)\n",
      "epoch: 151\n",
      "loss-----tensor(2.1399, grad_fn=<MulBackward0>)\n",
      "epoch: 152\n",
      "loss-----tensor(1.9350, grad_fn=<MulBackward0>)\n",
      "epoch: 153\n",
      "loss-----tensor(1.9539, grad_fn=<MulBackward0>)\n",
      "epoch: 154\n",
      "loss-----tensor(1.8742, grad_fn=<MulBackward0>)\n",
      "epoch: 155\n",
      "loss-----tensor(1.9533, grad_fn=<MulBackward0>)\n",
      "epoch: 156\n",
      "loss-----tensor(1.8680, grad_fn=<MulBackward0>)\n",
      "epoch: 157\n",
      "loss-----tensor(1.8438, grad_fn=<MulBackward0>)\n",
      "epoch: 158\n",
      "loss-----tensor(1.8588, grad_fn=<MulBackward0>)\n",
      "epoch: 159\n",
      "loss-----tensor(1.8260, grad_fn=<MulBackward0>)\n",
      "epoch: 160\n",
      "loss-----tensor(1.9151, grad_fn=<MulBackward0>)\n",
      "epoch: 161\n",
      "loss-----tensor(1.7796, grad_fn=<MulBackward0>)\n",
      "epoch: 162\n",
      "loss-----tensor(1.7406, grad_fn=<MulBackward0>)\n",
      "epoch: 163\n",
      "loss-----tensor(1.7388, grad_fn=<MulBackward0>)\n",
      "epoch: 164\n",
      "loss-----tensor(1.7881, grad_fn=<MulBackward0>)\n",
      "epoch: 165\n",
      "loss-----tensor(1.6909, grad_fn=<MulBackward0>)\n",
      "epoch: 166\n",
      "loss-----tensor(1.7276, grad_fn=<MulBackward0>)\n",
      "epoch: 167\n",
      "loss-----tensor(1.8033, grad_fn=<MulBackward0>)\n",
      "epoch: 168\n",
      "loss-----tensor(1.7604, grad_fn=<MulBackward0>)\n",
      "epoch: 169\n",
      "loss-----tensor(1.6374, grad_fn=<MulBackward0>)\n",
      "epoch: 170\n",
      "loss-----tensor(1.6048, grad_fn=<MulBackward0>)\n",
      "epoch: 171\n",
      "loss-----tensor(1.6329, grad_fn=<MulBackward0>)\n",
      "epoch: 172\n",
      "loss-----tensor(1.6417, grad_fn=<MulBackward0>)\n",
      "epoch: 173\n",
      "loss-----tensor(1.5572, grad_fn=<MulBackward0>)\n",
      "epoch: 174\n",
      "loss-----tensor(1.5412, grad_fn=<MulBackward0>)\n",
      "epoch: 175\n",
      "loss-----tensor(1.6358, grad_fn=<MulBackward0>)\n",
      "epoch: 176\n",
      "loss-----tensor(1.5214, grad_fn=<MulBackward0>)\n",
      "epoch: 177\n",
      "loss-----tensor(1.5171, grad_fn=<MulBackward0>)\n",
      "epoch: 178\n",
      "loss-----tensor(1.5030, grad_fn=<MulBackward0>)\n",
      "epoch: 179\n",
      "loss-----tensor(1.4909, grad_fn=<MulBackward0>)\n",
      "epoch: 180\n",
      "loss-----tensor(1.6516, grad_fn=<MulBackward0>)\n",
      "epoch: 181\n",
      "loss-----tensor(1.4788, grad_fn=<MulBackward0>)\n",
      "epoch: 182\n",
      "loss-----tensor(1.5796, grad_fn=<MulBackward0>)\n",
      "epoch: 183\n",
      "loss-----tensor(1.4243, grad_fn=<MulBackward0>)\n",
      "epoch: 184\n",
      "loss-----tensor(1.4323, grad_fn=<MulBackward0>)\n",
      "epoch: 185\n",
      "loss-----tensor(1.5060, grad_fn=<MulBackward0>)\n",
      "epoch: 186\n",
      "loss-----tensor(1.3893, grad_fn=<MulBackward0>)\n",
      "epoch: 187\n",
      "loss-----tensor(1.3626, grad_fn=<MulBackward0>)\n",
      "epoch: 188\n",
      "loss-----tensor(1.3571, grad_fn=<MulBackward0>)\n",
      "epoch: 189\n",
      "loss-----tensor(1.4136, grad_fn=<MulBackward0>)\n",
      "epoch: 190\n",
      "loss-----tensor(1.4256, grad_fn=<MulBackward0>)\n",
      "epoch: 191\n",
      "loss-----tensor(1.3699, grad_fn=<MulBackward0>)\n",
      "epoch: 192\n",
      "loss-----tensor(1.6358, grad_fn=<MulBackward0>)\n",
      "epoch: 193\n",
      "loss-----tensor(1.4169, grad_fn=<MulBackward0>)\n",
      "epoch: 194\n",
      "loss-----tensor(1.3064, grad_fn=<MulBackward0>)\n",
      "epoch: 195\n",
      "loss-----tensor(1.2897, grad_fn=<MulBackward0>)\n",
      "epoch: 196\n",
      "loss-----tensor(1.3565, grad_fn=<MulBackward0>)\n",
      "epoch: 197\n",
      "loss-----tensor(1.2798, grad_fn=<MulBackward0>)\n",
      "epoch: 198\n",
      "loss-----tensor(1.3115, grad_fn=<MulBackward0>)\n",
      "epoch: 199\n",
      "loss-----tensor(1.3901, grad_fn=<MulBackward0>)\n",
      "epoch: 200\n",
      "loss-----tensor(1.3377, grad_fn=<MulBackward0>)\n",
      "epoch: 201\n",
      "loss-----tensor(1.2270, grad_fn=<MulBackward0>)\n",
      "epoch: 202\n",
      "loss-----tensor(1.2089, grad_fn=<MulBackward0>)\n",
      "epoch: 203\n",
      "loss-----tensor(1.2365, grad_fn=<MulBackward0>)\n",
      "epoch: 204\n",
      "loss-----tensor(1.1774, grad_fn=<MulBackward0>)\n",
      "epoch: 205\n",
      "loss-----tensor(1.1952, grad_fn=<MulBackward0>)\n",
      "epoch: 206\n",
      "loss-----tensor(1.2060, grad_fn=<MulBackward0>)\n",
      "epoch: 207\n",
      "loss-----tensor(1.1691, grad_fn=<MulBackward0>)\n",
      "epoch: 208\n",
      "loss-----tensor(1.2668, grad_fn=<MulBackward0>)\n",
      "epoch: 209\n",
      "loss-----tensor(1.3212, grad_fn=<MulBackward0>)\n",
      "epoch: 210\n",
      "loss-----tensor(1.1604, grad_fn=<MulBackward0>)\n",
      "epoch: 211\n",
      "loss-----tensor(1.1485, grad_fn=<MulBackward0>)\n",
      "epoch: 212\n",
      "loss-----tensor(1.1642, grad_fn=<MulBackward0>)\n",
      "epoch: 213\n",
      "loss-----tensor(1.1081, grad_fn=<MulBackward0>)\n",
      "epoch: 214\n",
      "loss-----tensor(1.1618, grad_fn=<MulBackward0>)\n",
      "epoch: 215\n",
      "loss-----tensor(1.2690, grad_fn=<MulBackward0>)\n",
      "epoch: 216\n",
      "loss-----tensor(1.0868, grad_fn=<MulBackward0>)\n",
      "epoch: 217\n",
      "loss-----tensor(1.0800, grad_fn=<MulBackward0>)\n",
      "epoch: 218\n",
      "loss-----tensor(1.1494, grad_fn=<MulBackward0>)\n",
      "epoch: 219\n",
      "loss-----tensor(1.0584, grad_fn=<MulBackward0>)\n",
      "epoch: 220\n",
      "loss-----tensor(1.1588, grad_fn=<MulBackward0>)\n",
      "epoch: 221\n",
      "loss-----tensor(1.0221, grad_fn=<MulBackward0>)\n",
      "epoch: 222\n",
      "loss-----tensor(1.1734, grad_fn=<MulBackward0>)\n",
      "epoch: 223\n",
      "loss-----tensor(1.0664, grad_fn=<MulBackward0>)\n",
      "epoch: 224\n",
      "loss-----tensor(1.0202, grad_fn=<MulBackward0>)\n",
      "epoch: 225\n",
      "loss-----tensor(1.0726, grad_fn=<MulBackward0>)\n",
      "epoch: 226\n",
      "loss-----tensor(1.0114, grad_fn=<MulBackward0>)\n",
      "epoch: 227\n",
      "loss-----tensor(1.0553, grad_fn=<MulBackward0>)\n",
      "epoch: 228\n",
      "loss-----tensor(1.0059, grad_fn=<MulBackward0>)\n",
      "epoch: 229\n",
      "loss-----tensor(0.9991, grad_fn=<MulBackward0>)\n",
      "epoch: 230\n",
      "loss-----tensor(0.9880, grad_fn=<MulBackward0>)\n",
      "epoch: 231\n",
      "loss-----tensor(1.1807, grad_fn=<MulBackward0>)\n",
      "epoch: 232\n",
      "loss-----tensor(0.9881, grad_fn=<MulBackward0>)\n",
      "epoch: 233\n",
      "loss-----tensor(1.1043, grad_fn=<MulBackward0>)\n",
      "epoch: 234\n",
      "loss-----tensor(1.0343, grad_fn=<MulBackward0>)\n",
      "epoch: 235\n",
      "loss-----tensor(0.9392, grad_fn=<MulBackward0>)\n",
      "epoch: 236\n",
      "loss-----tensor(0.9513, grad_fn=<MulBackward0>)\n",
      "epoch: 237\n",
      "loss-----tensor(0.9500, grad_fn=<MulBackward0>)\n",
      "epoch: 238\n",
      "loss-----tensor(0.9192, grad_fn=<MulBackward0>)\n",
      "epoch: 239\n",
      "loss-----tensor(1.0403, grad_fn=<MulBackward0>)\n",
      "epoch: 240\n",
      "loss-----tensor(0.9840, grad_fn=<MulBackward0>)\n",
      "epoch: 241\n",
      "loss-----tensor(0.8977, grad_fn=<MulBackward0>)\n",
      "epoch: 242\n",
      "loss-----tensor(0.8963, grad_fn=<MulBackward0>)\n",
      "epoch: 243\n",
      "loss-----tensor(0.9026, grad_fn=<MulBackward0>)\n",
      "epoch: 244\n",
      "loss-----tensor(0.9043, grad_fn=<MulBackward0>)\n",
      "epoch: 245\n",
      "loss-----tensor(0.9085, grad_fn=<MulBackward0>)\n",
      "epoch: 246\n",
      "loss-----tensor(0.8998, grad_fn=<MulBackward0>)\n",
      "epoch: 247\n",
      "loss-----tensor(0.8536, grad_fn=<MulBackward0>)\n",
      "epoch: 248\n",
      "loss-----tensor(0.8486, grad_fn=<MulBackward0>)\n",
      "epoch: 249\n",
      "loss-----tensor(0.8996, grad_fn=<MulBackward0>)\n",
      "epoch: 250\n",
      "loss-----tensor(0.8554, grad_fn=<MulBackward0>)\n",
      "epoch: 251\n",
      "loss-----tensor(0.8785, grad_fn=<MulBackward0>)\n",
      "epoch: 252\n",
      "loss-----tensor(0.8162, grad_fn=<MulBackward0>)\n",
      "epoch: 253\n",
      "loss-----tensor(0.8121, grad_fn=<MulBackward0>)\n",
      "epoch: 254\n",
      "loss-----tensor(0.9501, grad_fn=<MulBackward0>)\n",
      "epoch: 255\n",
      "loss-----tensor(0.8186, grad_fn=<MulBackward0>)\n",
      "epoch: 256\n",
      "loss-----tensor(0.9062, grad_fn=<MulBackward0>)\n",
      "epoch: 257\n",
      "loss-----tensor(0.9048, grad_fn=<MulBackward0>)\n",
      "epoch: 258\n",
      "loss-----tensor(0.8173, grad_fn=<MulBackward0>)\n",
      "epoch: 259\n",
      "loss-----tensor(0.8624, grad_fn=<MulBackward0>)\n",
      "epoch: 260\n",
      "loss-----tensor(0.8123, grad_fn=<MulBackward0>)\n",
      "epoch: 261\n",
      "loss-----tensor(0.8582, grad_fn=<MulBackward0>)\n",
      "epoch: 262\n",
      "loss-----tensor(0.7778, grad_fn=<MulBackward0>)\n",
      "epoch: 263\n",
      "loss-----tensor(0.9127, grad_fn=<MulBackward0>)\n",
      "epoch: 264\n",
      "loss-----tensor(0.8059, grad_fn=<MulBackward0>)\n",
      "epoch: 265\n",
      "loss-----tensor(0.7455, grad_fn=<MulBackward0>)\n",
      "epoch: 266\n",
      "loss-----tensor(0.8298, grad_fn=<MulBackward0>)\n",
      "epoch: 267\n",
      "loss-----tensor(0.7440, grad_fn=<MulBackward0>)\n",
      "epoch: 268\n",
      "loss-----tensor(0.7262, grad_fn=<MulBackward0>)\n",
      "epoch: 269\n",
      "loss-----tensor(0.7155, grad_fn=<MulBackward0>)\n",
      "epoch: 270\n",
      "loss-----tensor(0.7778, grad_fn=<MulBackward0>)\n",
      "epoch: 271\n",
      "loss-----tensor(0.7229, grad_fn=<MulBackward0>)\n",
      "epoch: 272\n",
      "loss-----tensor(0.7192, grad_fn=<MulBackward0>)\n",
      "epoch: 273\n",
      "loss-----tensor(0.7043, grad_fn=<MulBackward0>)\n",
      "epoch: 274\n",
      "loss-----tensor(0.7070, grad_fn=<MulBackward0>)\n",
      "epoch: 275\n",
      "loss-----tensor(0.7308, grad_fn=<MulBackward0>)\n",
      "epoch: 276\n",
      "loss-----tensor(0.7337, grad_fn=<MulBackward0>)\n",
      "epoch: 277\n",
      "loss-----tensor(0.6973, grad_fn=<MulBackward0>)\n",
      "epoch: 278\n",
      "loss-----tensor(0.6760, grad_fn=<MulBackward0>)\n",
      "epoch: 279\n",
      "loss-----tensor(0.7335, grad_fn=<MulBackward0>)\n",
      "epoch: 280\n",
      "loss-----tensor(0.7337, grad_fn=<MulBackward0>)\n",
      "epoch: 281\n",
      "loss-----tensor(0.7061, grad_fn=<MulBackward0>)\n",
      "epoch: 282\n",
      "loss-----tensor(0.6862, grad_fn=<MulBackward0>)\n",
      "epoch: 283\n",
      "loss-----tensor(0.6579, grad_fn=<MulBackward0>)\n",
      "epoch: 284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss-----tensor(0.6906, grad_fn=<MulBackward0>)\n",
      "epoch: 285\n",
      "loss-----tensor(0.6558, grad_fn=<MulBackward0>)\n",
      "epoch: 286\n",
      "loss-----tensor(0.7477, grad_fn=<MulBackward0>)\n",
      "epoch: 287\n",
      "loss-----tensor(0.7801, grad_fn=<MulBackward0>)\n",
      "epoch: 288\n",
      "loss-----tensor(0.6462, grad_fn=<MulBackward0>)\n",
      "epoch: 289\n",
      "loss-----tensor(0.6383, grad_fn=<MulBackward0>)\n",
      "epoch: 290\n",
      "loss-----tensor(0.6563, grad_fn=<MulBackward0>)\n",
      "epoch: 291\n",
      "loss-----tensor(0.6639, grad_fn=<MulBackward0>)\n",
      "epoch: 292\n",
      "loss-----tensor(0.6240, grad_fn=<MulBackward0>)\n",
      "epoch: 293\n",
      "loss-----tensor(0.6467, grad_fn=<MulBackward0>)\n",
      "epoch: 294\n",
      "loss-----tensor(0.6400, grad_fn=<MulBackward0>)\n",
      "epoch: 295\n",
      "loss-----tensor(0.6079, grad_fn=<MulBackward0>)\n",
      "epoch: 296\n",
      "loss-----tensor(0.6865, grad_fn=<MulBackward0>)\n",
      "epoch: 297\n",
      "loss-----tensor(0.5923, grad_fn=<MulBackward0>)\n",
      "epoch: 298\n",
      "loss-----tensor(0.5952, grad_fn=<MulBackward0>)\n",
      "epoch: 299\n",
      "loss-----tensor(0.8853, grad_fn=<MulBackward0>)\n",
      "epoch: 300\n",
      "loss-----tensor(0.7026, grad_fn=<MulBackward0>)\n",
      "epoch: 301\n",
      "loss-----tensor(0.5824, grad_fn=<MulBackward0>)\n",
      "epoch: 302\n",
      "loss-----tensor(0.5791, grad_fn=<MulBackward0>)\n",
      "epoch: 303\n",
      "loss-----tensor(0.5975, grad_fn=<MulBackward0>)\n",
      "epoch: 304\n",
      "loss-----tensor(0.6096, grad_fn=<MulBackward0>)\n",
      "epoch: 305\n",
      "loss-----tensor(0.6611, grad_fn=<MulBackward0>)\n",
      "epoch: 306\n",
      "loss-----tensor(0.6675, grad_fn=<MulBackward0>)\n",
      "epoch: 307\n",
      "loss-----tensor(0.5580, grad_fn=<MulBackward0>)\n",
      "epoch: 308\n",
      "loss-----tensor(0.5678, grad_fn=<MulBackward0>)\n",
      "epoch: 309\n",
      "loss-----tensor(0.6127, grad_fn=<MulBackward0>)\n",
      "epoch: 310\n",
      "loss-----tensor(0.5758, grad_fn=<MulBackward0>)\n",
      "epoch: 311\n",
      "loss-----tensor(0.5510, grad_fn=<MulBackward0>)\n",
      "epoch: 312\n",
      "loss-----tensor(0.5471, grad_fn=<MulBackward0>)\n",
      "epoch: 313\n",
      "loss-----tensor(0.6343, grad_fn=<MulBackward0>)\n",
      "epoch: 314\n",
      "loss-----tensor(0.5626, grad_fn=<MulBackward0>)\n",
      "epoch: 315\n",
      "loss-----tensor(0.6188, grad_fn=<MulBackward0>)\n",
      "epoch: 316\n",
      "loss-----tensor(0.5869, grad_fn=<MulBackward0>)\n",
      "epoch: 317\n",
      "loss-----tensor(0.5285, grad_fn=<MulBackward0>)\n",
      "epoch: 318\n",
      "loss-----tensor(0.5768, grad_fn=<MulBackward0>)\n",
      "epoch: 319\n",
      "loss-----tensor(0.5667, grad_fn=<MulBackward0>)\n",
      "epoch: 320\n",
      "loss-----tensor(0.5569, grad_fn=<MulBackward0>)\n",
      "epoch: 321\n",
      "loss-----tensor(0.5348, grad_fn=<MulBackward0>)\n",
      "epoch: 322\n",
      "loss-----tensor(0.5288, grad_fn=<MulBackward0>)\n",
      "epoch: 323\n",
      "loss-----tensor(0.5150, grad_fn=<MulBackward0>)\n",
      "epoch: 324\n",
      "loss-----tensor(0.6088, grad_fn=<MulBackward0>)\n",
      "epoch: 325\n",
      "loss-----tensor(0.5141, grad_fn=<MulBackward0>)\n",
      "epoch: 326\n",
      "loss-----tensor(0.5259, grad_fn=<MulBackward0>)\n",
      "epoch: 327\n",
      "loss-----tensor(0.5033, grad_fn=<MulBackward0>)\n",
      "epoch: 328\n",
      "loss-----tensor(0.5121, grad_fn=<MulBackward0>)\n",
      "epoch: 329\n",
      "loss-----tensor(0.5011, grad_fn=<MulBackward0>)\n",
      "epoch: 330\n",
      "loss-----tensor(0.4800, grad_fn=<MulBackward0>)\n",
      "epoch: 331\n",
      "loss-----tensor(0.5749, grad_fn=<MulBackward0>)\n",
      "epoch: 332\n",
      "loss-----tensor(0.4910, grad_fn=<MulBackward0>)\n",
      "epoch: 333\n",
      "loss-----tensor(0.6121, grad_fn=<MulBackward0>)\n",
      "epoch: 334\n",
      "loss-----tensor(0.4929, grad_fn=<MulBackward0>)\n",
      "epoch: 335\n",
      "loss-----tensor(0.4761, grad_fn=<MulBackward0>)\n",
      "epoch: 336\n",
      "loss-----tensor(0.4703, grad_fn=<MulBackward0>)\n",
      "epoch: 337\n",
      "loss-----tensor(0.4776, grad_fn=<MulBackward0>)\n",
      "epoch: 338\n",
      "loss-----tensor(0.4828, grad_fn=<MulBackward0>)\n",
      "epoch: 339\n",
      "loss-----tensor(0.4938, grad_fn=<MulBackward0>)\n",
      "epoch: 340\n",
      "loss-----tensor(0.4674, grad_fn=<MulBackward0>)\n",
      "epoch: 341\n",
      "loss-----tensor(0.4863, grad_fn=<MulBackward0>)\n",
      "epoch: 342\n",
      "loss-----tensor(0.4740, grad_fn=<MulBackward0>)\n",
      "epoch: 343\n",
      "loss-----tensor(0.5010, grad_fn=<MulBackward0>)\n",
      "epoch: 344\n",
      "loss-----tensor(0.4554, grad_fn=<MulBackward0>)\n",
      "epoch: 345\n",
      "loss-----tensor(0.4515, grad_fn=<MulBackward0>)\n",
      "epoch: 346\n",
      "loss-----tensor(0.4764, grad_fn=<MulBackward0>)\n",
      "epoch: 347\n",
      "loss-----tensor(0.4424, grad_fn=<MulBackward0>)\n",
      "epoch: 348\n",
      "loss-----tensor(0.4701, grad_fn=<MulBackward0>)\n",
      "epoch: 349\n",
      "loss-----tensor(0.4841, grad_fn=<MulBackward0>)\n",
      "epoch: 350\n",
      "loss-----tensor(0.4504, grad_fn=<MulBackward0>)\n",
      "epoch: 351\n",
      "loss-----tensor(0.4762, grad_fn=<MulBackward0>)\n",
      "epoch: 352\n",
      "loss-----tensor(0.4925, grad_fn=<MulBackward0>)\n",
      "epoch: 353\n",
      "loss-----tensor(0.4374, grad_fn=<MulBackward0>)\n",
      "epoch: 354\n",
      "loss-----tensor(0.4296, grad_fn=<MulBackward0>)\n",
      "epoch: 355\n",
      "loss-----tensor(0.4308, grad_fn=<MulBackward0>)\n",
      "epoch: 356\n",
      "loss-----tensor(0.4276, grad_fn=<MulBackward0>)\n",
      "epoch: 357\n",
      "loss-----tensor(0.4946, grad_fn=<MulBackward0>)\n",
      "epoch: 358\n",
      "loss-----tensor(0.4587, grad_fn=<MulBackward0>)\n",
      "epoch: 359\n",
      "loss-----tensor(0.4163, grad_fn=<MulBackward0>)\n",
      "epoch: 360\n",
      "loss-----tensor(0.4688, grad_fn=<MulBackward0>)\n",
      "epoch: 361\n",
      "loss-----tensor(0.4077, grad_fn=<MulBackward0>)\n",
      "epoch: 362\n",
      "loss-----tensor(0.4438, grad_fn=<MulBackward0>)\n",
      "epoch: 363\n",
      "loss-----tensor(0.4104, grad_fn=<MulBackward0>)\n",
      "epoch: 364\n",
      "loss-----tensor(0.3997, grad_fn=<MulBackward0>)\n",
      "epoch: 365\n",
      "loss-----tensor(0.4136, grad_fn=<MulBackward0>)\n",
      "epoch: 366\n",
      "loss-----tensor(0.4001, grad_fn=<MulBackward0>)\n",
      "epoch: 367\n",
      "loss-----tensor(0.4231, grad_fn=<MulBackward0>)\n",
      "epoch: 368\n",
      "loss-----tensor(0.4050, grad_fn=<MulBackward0>)\n",
      "epoch: 369\n",
      "loss-----tensor(0.3913, grad_fn=<MulBackward0>)\n",
      "epoch: 370\n",
      "loss-----tensor(0.3909, grad_fn=<MulBackward0>)\n",
      "epoch: 371\n",
      "loss-----tensor(0.3918, grad_fn=<MulBackward0>)\n",
      "epoch: 372\n",
      "loss-----tensor(0.4095, grad_fn=<MulBackward0>)\n",
      "epoch: 373\n",
      "loss-----tensor(0.3916, grad_fn=<MulBackward0>)\n",
      "epoch: 374\n",
      "loss-----tensor(0.3887, grad_fn=<MulBackward0>)\n",
      "epoch: 375\n",
      "loss-----tensor(0.3956, grad_fn=<MulBackward0>)\n",
      "epoch: 376\n",
      "loss-----tensor(0.3967, grad_fn=<MulBackward0>)\n",
      "epoch: 377\n",
      "loss-----tensor(0.4300, grad_fn=<MulBackward0>)\n",
      "epoch: 378\n",
      "loss-----tensor(0.3905, grad_fn=<MulBackward0>)\n",
      "epoch: 379\n",
      "loss-----tensor(0.3891, grad_fn=<MulBackward0>)\n",
      "epoch: 380\n",
      "loss-----tensor(0.3829, grad_fn=<MulBackward0>)\n",
      "epoch: 381\n",
      "loss-----tensor(0.4005, grad_fn=<MulBackward0>)\n",
      "epoch: 382\n",
      "loss-----tensor(0.3889, grad_fn=<MulBackward0>)\n",
      "epoch: 383\n",
      "loss-----tensor(0.3865, grad_fn=<MulBackward0>)\n",
      "epoch: 384\n",
      "loss-----tensor(0.4620, grad_fn=<MulBackward0>)\n",
      "epoch: 385\n",
      "loss-----tensor(0.3687, grad_fn=<MulBackward0>)\n",
      "epoch: 386\n",
      "loss-----tensor(0.3800, grad_fn=<MulBackward0>)\n",
      "epoch: 387\n",
      "loss-----tensor(0.3616, grad_fn=<MulBackward0>)\n",
      "epoch: 388\n",
      "loss-----tensor(0.3703, grad_fn=<MulBackward0>)\n",
      "epoch: 389\n",
      "loss-----tensor(0.3629, grad_fn=<MulBackward0>)\n",
      "epoch: 390\n",
      "loss-----tensor(0.3556, grad_fn=<MulBackward0>)\n",
      "epoch: 391\n",
      "loss-----tensor(0.3644, grad_fn=<MulBackward0>)\n",
      "epoch: 392\n",
      "loss-----tensor(0.3689, grad_fn=<MulBackward0>)\n",
      "epoch: 393\n",
      "loss-----tensor(0.3701, grad_fn=<MulBackward0>)\n",
      "epoch: 394\n",
      "loss-----tensor(0.3425, grad_fn=<MulBackward0>)\n",
      "epoch: 395\n",
      "loss-----tensor(0.3457, grad_fn=<MulBackward0>)\n",
      "epoch: 396\n",
      "loss-----tensor(0.3615, grad_fn=<MulBackward0>)\n",
      "epoch: 397\n",
      "loss-----tensor(0.4466, grad_fn=<MulBackward0>)\n",
      "epoch: 398\n",
      "loss-----tensor(0.3582, grad_fn=<MulBackward0>)\n",
      "epoch: 399\n",
      "loss-----tensor(0.3850, grad_fn=<MulBackward0>)\n",
      "epoch: 400\n",
      "loss-----tensor(0.3388, grad_fn=<MulBackward0>)\n",
      "epoch: 401\n",
      "loss-----tensor(0.3319, grad_fn=<MulBackward0>)\n",
      "epoch: 402\n",
      "loss-----tensor(0.3342, grad_fn=<MulBackward0>)\n",
      "epoch: 403\n",
      "loss-----tensor(0.3665, grad_fn=<MulBackward0>)\n",
      "epoch: 404\n",
      "loss-----tensor(0.3289, grad_fn=<MulBackward0>)\n",
      "epoch: 405\n",
      "loss-----tensor(0.3339, grad_fn=<MulBackward0>)\n",
      "epoch: 406\n",
      "loss-----tensor(0.3233, grad_fn=<MulBackward0>)\n",
      "epoch: 407\n",
      "loss-----tensor(0.4344, grad_fn=<MulBackward0>)\n",
      "epoch: 408\n",
      "loss-----tensor(0.3296, grad_fn=<MulBackward0>)\n",
      "epoch: 409\n",
      "loss-----tensor(0.3306, grad_fn=<MulBackward0>)\n",
      "epoch: 410\n",
      "loss-----tensor(0.4249, grad_fn=<MulBackward0>)\n",
      "epoch: 411\n",
      "loss-----tensor(0.3420, grad_fn=<MulBackward0>)\n",
      "epoch: 412\n",
      "loss-----tensor(0.3148, grad_fn=<MulBackward0>)\n",
      "epoch: 413\n",
      "loss-----tensor(0.3249, grad_fn=<MulBackward0>)\n",
      "epoch: 414\n",
      "loss-----tensor(0.3265, grad_fn=<MulBackward0>)\n",
      "epoch: 415\n",
      "loss-----tensor(0.3303, grad_fn=<MulBackward0>)\n",
      "epoch: 416\n",
      "loss-----tensor(0.3116, grad_fn=<MulBackward0>)\n",
      "epoch: 417\n",
      "loss-----tensor(0.3249, grad_fn=<MulBackward0>)\n",
      "epoch: 418\n",
      "loss-----tensor(0.3102, grad_fn=<MulBackward0>)\n",
      "epoch: 419\n",
      "loss-----tensor(0.3063, grad_fn=<MulBackward0>)\n",
      "epoch: 420\n",
      "loss-----tensor(0.3490, grad_fn=<MulBackward0>)\n",
      "epoch: 421\n",
      "loss-----tensor(0.3250, grad_fn=<MulBackward0>)\n",
      "epoch: 422\n",
      "loss-----tensor(0.3343, grad_fn=<MulBackward0>)\n",
      "epoch: 423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss-----tensor(0.2982, grad_fn=<MulBackward0>)\n",
      "epoch: 424\n",
      "loss-----tensor(0.3076, grad_fn=<MulBackward0>)\n",
      "epoch: 425\n",
      "loss-----tensor(0.2994, grad_fn=<MulBackward0>)\n",
      "epoch: 426\n",
      "loss-----tensor(0.3053, grad_fn=<MulBackward0>)\n",
      "epoch: 427\n",
      "loss-----tensor(0.3727, grad_fn=<MulBackward0>)\n",
      "epoch: 428\n",
      "loss-----tensor(0.3296, grad_fn=<MulBackward0>)\n",
      "epoch: 429\n",
      "loss-----tensor(0.3033, grad_fn=<MulBackward0>)\n",
      "epoch: 430\n",
      "loss-----tensor(0.2947, grad_fn=<MulBackward0>)\n",
      "epoch: 431\n",
      "loss-----tensor(0.3003, grad_fn=<MulBackward0>)\n",
      "epoch: 432\n",
      "loss-----tensor(0.3049, grad_fn=<MulBackward0>)\n",
      "epoch: 433\n",
      "loss-----tensor(0.2837, grad_fn=<MulBackward0>)\n",
      "epoch: 434\n",
      "loss-----tensor(0.3073, grad_fn=<MulBackward0>)\n",
      "epoch: 435\n",
      "loss-----tensor(0.2976, grad_fn=<MulBackward0>)\n",
      "epoch: 436\n",
      "loss-----tensor(0.3466, grad_fn=<MulBackward0>)\n",
      "epoch: 437\n",
      "loss-----tensor(0.2904, grad_fn=<MulBackward0>)\n",
      "epoch: 438\n",
      "loss-----tensor(0.2891, grad_fn=<MulBackward0>)\n",
      "epoch: 439\n",
      "loss-----tensor(0.2907, grad_fn=<MulBackward0>)\n",
      "epoch: 440\n",
      "loss-----tensor(0.2942, grad_fn=<MulBackward0>)\n",
      "epoch: 441\n",
      "loss-----tensor(0.2949, grad_fn=<MulBackward0>)\n",
      "epoch: 442\n",
      "loss-----tensor(0.3244, grad_fn=<MulBackward0>)\n",
      "epoch: 443\n",
      "loss-----tensor(0.2780, grad_fn=<MulBackward0>)\n",
      "epoch: 444\n",
      "loss-----tensor(0.2810, grad_fn=<MulBackward0>)\n",
      "epoch: 445\n",
      "loss-----tensor(0.3155, grad_fn=<MulBackward0>)\n",
      "epoch: 446\n",
      "loss-----tensor(0.3000, grad_fn=<MulBackward0>)\n",
      "epoch: 447\n",
      "loss-----tensor(0.2788, grad_fn=<MulBackward0>)\n",
      "epoch: 448\n",
      "loss-----tensor(0.2740, grad_fn=<MulBackward0>)\n",
      "epoch: 449\n",
      "loss-----tensor(0.2847, grad_fn=<MulBackward0>)\n",
      "epoch: 450\n",
      "loss-----tensor(0.2793, grad_fn=<MulBackward0>)\n",
      "epoch: 451\n",
      "loss-----tensor(0.2707, grad_fn=<MulBackward0>)\n",
      "epoch: 452\n",
      "loss-----tensor(0.2690, grad_fn=<MulBackward0>)\n",
      "epoch: 453\n",
      "loss-----tensor(0.2827, grad_fn=<MulBackward0>)\n",
      "epoch: 454\n",
      "loss-----tensor(0.2658, grad_fn=<MulBackward0>)\n",
      "epoch: 455\n",
      "loss-----tensor(0.2678, grad_fn=<MulBackward0>)\n",
      "epoch: 456\n",
      "loss-----tensor(0.2673, grad_fn=<MulBackward0>)\n",
      "epoch: 457\n",
      "loss-----tensor(0.2624, grad_fn=<MulBackward0>)\n",
      "epoch: 458\n",
      "loss-----tensor(0.2563, grad_fn=<MulBackward0>)\n",
      "epoch: 459\n",
      "loss-----tensor(0.2746, grad_fn=<MulBackward0>)\n",
      "epoch: 460\n",
      "loss-----tensor(0.2629, grad_fn=<MulBackward0>)\n",
      "epoch: 461\n",
      "loss-----tensor(0.3146, grad_fn=<MulBackward0>)\n",
      "epoch: 462\n",
      "loss-----tensor(0.2599, grad_fn=<MulBackward0>)\n",
      "epoch: 463\n",
      "loss-----tensor(0.2794, grad_fn=<MulBackward0>)\n",
      "epoch: 464\n",
      "loss-----tensor(0.2709, grad_fn=<MulBackward0>)\n",
      "epoch: 465\n",
      "loss-----tensor(0.2585, grad_fn=<MulBackward0>)\n",
      "epoch: 466\n",
      "loss-----tensor(0.2639, grad_fn=<MulBackward0>)\n",
      "epoch: 467\n",
      "loss-----tensor(0.2803, grad_fn=<MulBackward0>)\n",
      "epoch: 468\n",
      "loss-----tensor(0.2603, grad_fn=<MulBackward0>)\n",
      "epoch: 469\n",
      "loss-----tensor(0.2680, grad_fn=<MulBackward0>)\n",
      "epoch: 470\n",
      "loss-----tensor(0.2536, grad_fn=<MulBackward0>)\n",
      "epoch: 471\n",
      "loss-----tensor(0.2448, grad_fn=<MulBackward0>)\n",
      "epoch: 472\n",
      "loss-----tensor(0.2766, grad_fn=<MulBackward0>)\n",
      "epoch: 473\n",
      "loss-----tensor(0.2477, grad_fn=<MulBackward0>)\n",
      "epoch: 474\n",
      "loss-----tensor(0.2526, grad_fn=<MulBackward0>)\n",
      "epoch: 475\n",
      "loss-----tensor(0.2478, grad_fn=<MulBackward0>)\n",
      "epoch: 476\n",
      "loss-----tensor(0.2498, grad_fn=<MulBackward0>)\n",
      "epoch: 477\n",
      "loss-----tensor(0.2448, grad_fn=<MulBackward0>)\n",
      "epoch: 478\n",
      "loss-----tensor(0.2582, grad_fn=<MulBackward0>)\n",
      "epoch: 479\n",
      "loss-----tensor(0.2572, grad_fn=<MulBackward0>)\n",
      "epoch: 480\n",
      "loss-----tensor(0.2547, grad_fn=<MulBackward0>)\n",
      "epoch: 481\n",
      "loss-----tensor(0.2566, grad_fn=<MulBackward0>)\n",
      "epoch: 482\n",
      "loss-----tensor(0.2522, grad_fn=<MulBackward0>)\n",
      "epoch: 483\n",
      "loss-----tensor(0.2419, grad_fn=<MulBackward0>)\n",
      "epoch: 484\n",
      "loss-----tensor(0.2518, grad_fn=<MulBackward0>)\n",
      "epoch: 485\n",
      "loss-----tensor(0.2405, grad_fn=<MulBackward0>)\n",
      "epoch: 486\n",
      "loss-----tensor(0.2339, grad_fn=<MulBackward0>)\n",
      "epoch: 487\n",
      "loss-----tensor(0.2716, grad_fn=<MulBackward0>)\n",
      "epoch: 488\n",
      "loss-----tensor(0.2324, grad_fn=<MulBackward0>)\n",
      "epoch: 489\n",
      "loss-----tensor(0.2444, grad_fn=<MulBackward0>)\n",
      "epoch: 490\n",
      "loss-----tensor(0.2560, grad_fn=<MulBackward0>)\n",
      "epoch: 491\n",
      "loss-----tensor(0.2443, grad_fn=<MulBackward0>)\n",
      "epoch: 492\n",
      "loss-----tensor(0.2516, grad_fn=<MulBackward0>)\n",
      "epoch: 493\n",
      "loss-----tensor(0.2421, grad_fn=<MulBackward0>)\n",
      "epoch: 494\n",
      "loss-----tensor(0.3138, grad_fn=<MulBackward0>)\n",
      "epoch: 495\n",
      "loss-----tensor(0.2391, grad_fn=<MulBackward0>)\n",
      "epoch: 496\n",
      "loss-----tensor(0.2665, grad_fn=<MulBackward0>)\n",
      "epoch: 497\n",
      "loss-----tensor(0.2275, grad_fn=<MulBackward0>)\n",
      "epoch: 498\n",
      "loss-----tensor(0.2408, grad_fn=<MulBackward0>)\n",
      "epoch: 499\n",
      "loss-----tensor(0.2307, grad_fn=<MulBackward0>)\n",
      "epoch: 500\n",
      "loss-----tensor(0.2200, grad_fn=<MulBackward0>)\n",
      "epoch: 501\n",
      "loss-----tensor(0.2259, grad_fn=<MulBackward0>)\n",
      "epoch: 502\n",
      "loss-----tensor(0.2305, grad_fn=<MulBackward0>)\n",
      "epoch: 503\n",
      "loss-----tensor(0.2223, grad_fn=<MulBackward0>)\n",
      "epoch: 504\n",
      "loss-----tensor(0.2280, grad_fn=<MulBackward0>)\n",
      "epoch: 505\n",
      "loss-----tensor(0.2383, grad_fn=<MulBackward0>)\n",
      "epoch: 506\n",
      "loss-----tensor(0.2322, grad_fn=<MulBackward0>)\n",
      "epoch: 507\n",
      "loss-----tensor(0.2157, grad_fn=<MulBackward0>)\n",
      "epoch: 508\n",
      "loss-----tensor(0.2211, grad_fn=<MulBackward0>)\n",
      "epoch: 509\n",
      "loss-----tensor(0.2151, grad_fn=<MulBackward0>)\n",
      "epoch: 510\n",
      "loss-----tensor(0.2166, grad_fn=<MulBackward0>)\n",
      "epoch: 511\n",
      "loss-----tensor(0.2255, grad_fn=<MulBackward0>)\n",
      "epoch: 512\n",
      "loss-----tensor(0.2228, grad_fn=<MulBackward0>)\n",
      "epoch: 513\n",
      "loss-----tensor(0.2114, grad_fn=<MulBackward0>)\n",
      "epoch: 514\n",
      "loss-----tensor(0.2166, grad_fn=<MulBackward0>)\n",
      "epoch: 515\n",
      "loss-----tensor(0.2125, grad_fn=<MulBackward0>)\n",
      "epoch: 516\n",
      "loss-----tensor(0.2175, grad_fn=<MulBackward0>)\n",
      "epoch: 517\n",
      "loss-----tensor(0.2189, grad_fn=<MulBackward0>)\n",
      "epoch: 518\n",
      "loss-----tensor(0.2168, grad_fn=<MulBackward0>)\n",
      "epoch: 519\n",
      "loss-----tensor(0.2236, grad_fn=<MulBackward0>)\n",
      "epoch: 520\n",
      "loss-----tensor(0.2137, grad_fn=<MulBackward0>)\n",
      "epoch: 521\n",
      "loss-----tensor(0.2090, grad_fn=<MulBackward0>)\n",
      "epoch: 522\n",
      "loss-----tensor(0.2116, grad_fn=<MulBackward0>)\n",
      "epoch: 523\n",
      "loss-----tensor(0.2070, grad_fn=<MulBackward0>)\n",
      "epoch: 524\n",
      "loss-----tensor(0.2200, grad_fn=<MulBackward0>)\n",
      "epoch: 525\n",
      "loss-----tensor(0.2423, grad_fn=<MulBackward0>)\n",
      "epoch: 526\n",
      "loss-----tensor(0.2180, grad_fn=<MulBackward0>)\n",
      "epoch: 527\n",
      "loss-----tensor(0.2149, grad_fn=<MulBackward0>)\n",
      "epoch: 528\n",
      "loss-----tensor(0.2024, grad_fn=<MulBackward0>)\n",
      "epoch: 529\n",
      "loss-----tensor(0.2126, grad_fn=<MulBackward0>)\n",
      "epoch: 530\n",
      "loss-----tensor(0.2082, grad_fn=<MulBackward0>)\n",
      "epoch: 531\n",
      "loss-----tensor(0.1994, grad_fn=<MulBackward0>)\n",
      "epoch: 532\n",
      "loss-----tensor(0.2087, grad_fn=<MulBackward0>)\n",
      "epoch: 533\n",
      "loss-----tensor(0.2286, grad_fn=<MulBackward0>)\n",
      "epoch: 534\n",
      "loss-----tensor(0.1975, grad_fn=<MulBackward0>)\n",
      "epoch: 535\n",
      "loss-----tensor(0.2059, grad_fn=<MulBackward0>)\n",
      "epoch: 536\n",
      "loss-----tensor(0.2054, grad_fn=<MulBackward0>)\n",
      "epoch: 537\n",
      "loss-----tensor(0.1917, grad_fn=<MulBackward0>)\n",
      "epoch: 538\n",
      "loss-----tensor(0.2749, grad_fn=<MulBackward0>)\n",
      "epoch: 539\n",
      "loss-----tensor(0.1982, grad_fn=<MulBackward0>)\n",
      "epoch: 540\n",
      "loss-----tensor(0.1984, grad_fn=<MulBackward0>)\n",
      "epoch: 541\n",
      "loss-----tensor(0.1939, grad_fn=<MulBackward0>)\n",
      "epoch: 542\n",
      "loss-----tensor(0.1928, grad_fn=<MulBackward0>)\n",
      "epoch: 543\n",
      "loss-----tensor(0.2064, grad_fn=<MulBackward0>)\n",
      "epoch: 544\n",
      "loss-----tensor(0.1988, grad_fn=<MulBackward0>)\n",
      "epoch: 545\n",
      "loss-----tensor(0.1946, grad_fn=<MulBackward0>)\n",
      "epoch: 546\n",
      "loss-----tensor(0.1997, grad_fn=<MulBackward0>)\n",
      "epoch: 547\n",
      "loss-----tensor(0.2133, grad_fn=<MulBackward0>)\n",
      "epoch: 548\n",
      "loss-----tensor(0.1893, grad_fn=<MulBackward0>)\n",
      "epoch: 549\n",
      "loss-----tensor(0.1973, grad_fn=<MulBackward0>)\n",
      "epoch: 550\n",
      "loss-----tensor(0.1957, grad_fn=<MulBackward0>)\n",
      "epoch: 551\n",
      "loss-----tensor(0.1855, grad_fn=<MulBackward0>)\n",
      "epoch: 552\n",
      "loss-----tensor(0.2039, grad_fn=<MulBackward0>)\n",
      "epoch: 553\n",
      "loss-----tensor(0.1963, grad_fn=<MulBackward0>)\n",
      "epoch: 554\n",
      "loss-----tensor(0.1951, grad_fn=<MulBackward0>)\n",
      "epoch: 555\n",
      "loss-----tensor(0.2453, grad_fn=<MulBackward0>)\n",
      "epoch: 556\n",
      "loss-----tensor(0.1843, grad_fn=<MulBackward0>)\n",
      "epoch: 557\n",
      "loss-----tensor(0.1878, grad_fn=<MulBackward0>)\n",
      "epoch: 558\n",
      "loss-----tensor(0.1901, grad_fn=<MulBackward0>)\n",
      "epoch: 559\n",
      "loss-----tensor(0.1850, grad_fn=<MulBackward0>)\n",
      "epoch: 560\n",
      "loss-----tensor(0.1897, grad_fn=<MulBackward0>)\n",
      "epoch: 561\n",
      "loss-----tensor(0.2115, grad_fn=<MulBackward0>)\n",
      "epoch: 562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss-----tensor(0.1869, grad_fn=<MulBackward0>)\n",
      "epoch: 563\n",
      "loss-----tensor(0.1832, grad_fn=<MulBackward0>)\n",
      "epoch: 564\n",
      "loss-----tensor(0.2014, grad_fn=<MulBackward0>)\n",
      "epoch: 565\n",
      "loss-----tensor(0.2078, grad_fn=<MulBackward0>)\n",
      "epoch: 566\n",
      "loss-----tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "epoch: 567\n",
      "loss-----tensor(0.1772, grad_fn=<MulBackward0>)\n",
      "epoch: 568\n",
      "loss-----tensor(0.1905, grad_fn=<MulBackward0>)\n",
      "epoch: 569\n",
      "loss-----tensor(0.1829, grad_fn=<MulBackward0>)\n",
      "epoch: 570\n",
      "loss-----tensor(0.1791, grad_fn=<MulBackward0>)\n",
      "epoch: 571\n",
      "loss-----tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "epoch: 572\n",
      "loss-----tensor(0.1757, grad_fn=<MulBackward0>)\n",
      "epoch: 573\n",
      "loss-----tensor(0.2038, grad_fn=<MulBackward0>)\n",
      "epoch: 574\n",
      "loss-----tensor(0.1802, grad_fn=<MulBackward0>)\n",
      "epoch: 575\n",
      "loss-----tensor(0.1960, grad_fn=<MulBackward0>)\n",
      "epoch: 576\n",
      "loss-----tensor(0.1792, grad_fn=<MulBackward0>)\n",
      "epoch: 577\n",
      "loss-----tensor(0.1733, grad_fn=<MulBackward0>)\n",
      "epoch: 578\n",
      "loss-----tensor(0.1764, grad_fn=<MulBackward0>)\n",
      "epoch: 579\n",
      "loss-----tensor(0.1771, grad_fn=<MulBackward0>)\n",
      "epoch: 580\n",
      "loss-----tensor(0.1960, grad_fn=<MulBackward0>)\n",
      "epoch: 581\n",
      "loss-----tensor(0.1726, grad_fn=<MulBackward0>)\n",
      "epoch: 582\n",
      "loss-----tensor(0.1834, grad_fn=<MulBackward0>)\n",
      "epoch: 583\n",
      "loss-----tensor(0.1814, grad_fn=<MulBackward0>)\n",
      "epoch: 584\n",
      "loss-----tensor(0.1844, grad_fn=<MulBackward0>)\n",
      "epoch: 585\n",
      "loss-----tensor(0.1817, grad_fn=<MulBackward0>)\n",
      "epoch: 586\n",
      "loss-----tensor(0.1917, grad_fn=<MulBackward0>)\n",
      "epoch: 587\n",
      "loss-----tensor(0.1738, grad_fn=<MulBackward0>)\n",
      "epoch: 588\n",
      "loss-----tensor(0.1740, grad_fn=<MulBackward0>)\n",
      "epoch: 589\n",
      "loss-----tensor(0.1790, grad_fn=<MulBackward0>)\n",
      "epoch: 590\n",
      "loss-----tensor(0.1683, grad_fn=<MulBackward0>)\n",
      "epoch: 591\n",
      "loss-----tensor(0.1729, grad_fn=<MulBackward0>)\n",
      "epoch: 592\n",
      "loss-----tensor(0.1619, grad_fn=<MulBackward0>)\n",
      "epoch: 593\n",
      "loss-----tensor(0.1681, grad_fn=<MulBackward0>)\n",
      "epoch: 594\n",
      "loss-----tensor(0.1910, grad_fn=<MulBackward0>)\n",
      "epoch: 595\n",
      "loss-----tensor(0.1846, grad_fn=<MulBackward0>)\n",
      "epoch: 596\n",
      "loss-----tensor(0.1698, grad_fn=<MulBackward0>)\n",
      "epoch: 597\n",
      "loss-----tensor(0.1642, grad_fn=<MulBackward0>)\n",
      "epoch: 598\n",
      "loss-----tensor(0.1783, grad_fn=<MulBackward0>)\n",
      "epoch: 599\n",
      "loss-----tensor(0.1614, grad_fn=<MulBackward0>)\n",
      "epoch: 600\n",
      "loss-----tensor(0.1592, grad_fn=<MulBackward0>)\n",
      "epoch: 601\n",
      "loss-----tensor(0.1695, grad_fn=<MulBackward0>)\n",
      "epoch: 602\n",
      "loss-----tensor(0.1696, grad_fn=<MulBackward0>)\n",
      "epoch: 603\n",
      "loss-----tensor(0.1608, grad_fn=<MulBackward0>)\n",
      "epoch: 604\n",
      "loss-----tensor(0.1594, grad_fn=<MulBackward0>)\n",
      "epoch: 605\n",
      "loss-----tensor(0.1665, grad_fn=<MulBackward0>)\n",
      "epoch: 606\n",
      "loss-----tensor(0.1679, grad_fn=<MulBackward0>)\n",
      "epoch: 607\n",
      "loss-----tensor(0.1685, grad_fn=<MulBackward0>)\n",
      "epoch: 608\n",
      "loss-----tensor(0.1601, grad_fn=<MulBackward0>)\n",
      "epoch: 609\n",
      "loss-----tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "epoch: 610\n",
      "loss-----tensor(0.1870, grad_fn=<MulBackward0>)\n",
      "epoch: 611\n",
      "loss-----tensor(0.1604, grad_fn=<MulBackward0>)\n",
      "epoch: 612\n",
      "loss-----tensor(0.1873, grad_fn=<MulBackward0>)\n",
      "epoch: 613\n",
      "loss-----tensor(0.1611, grad_fn=<MulBackward0>)\n",
      "epoch: 614\n",
      "loss-----tensor(0.1809, grad_fn=<MulBackward0>)\n",
      "epoch: 615\n",
      "loss-----tensor(0.1747, grad_fn=<MulBackward0>)\n",
      "epoch: 616\n",
      "loss-----tensor(0.1595, grad_fn=<MulBackward0>)\n",
      "epoch: 617\n",
      "loss-----tensor(0.1601, grad_fn=<MulBackward0>)\n",
      "epoch: 618\n",
      "loss-----tensor(0.1579, grad_fn=<MulBackward0>)\n",
      "epoch: 619\n",
      "loss-----tensor(0.1769, grad_fn=<MulBackward0>)\n",
      "epoch: 620\n",
      "loss-----tensor(0.1621, grad_fn=<MulBackward0>)\n",
      "epoch: 621\n",
      "loss-----tensor(0.1861, grad_fn=<MulBackward0>)\n",
      "epoch: 622\n",
      "loss-----tensor(0.1548, grad_fn=<MulBackward0>)\n",
      "epoch: 623\n",
      "loss-----tensor(0.1530, grad_fn=<MulBackward0>)\n",
      "epoch: 624\n",
      "loss-----tensor(0.1566, grad_fn=<MulBackward0>)\n",
      "epoch: 625\n",
      "loss-----tensor(0.1728, grad_fn=<MulBackward0>)\n",
      "epoch: 626\n",
      "loss-----tensor(0.1524, grad_fn=<MulBackward0>)\n",
      "epoch: 627\n",
      "loss-----tensor(0.1504, grad_fn=<MulBackward0>)\n",
      "epoch: 628\n",
      "loss-----tensor(0.1541, grad_fn=<MulBackward0>)\n",
      "epoch: 629\n",
      "loss-----tensor(0.1622, grad_fn=<MulBackward0>)\n",
      "epoch: 630\n",
      "loss-----tensor(0.1548, grad_fn=<MulBackward0>)\n",
      "epoch: 631\n",
      "loss-----tensor(0.1493, grad_fn=<MulBackward0>)\n",
      "epoch: 632\n",
      "loss-----tensor(0.1577, grad_fn=<MulBackward0>)\n",
      "epoch: 633\n",
      "loss-----tensor(0.1638, grad_fn=<MulBackward0>)\n",
      "epoch: 634\n",
      "loss-----tensor(0.1703, grad_fn=<MulBackward0>)\n",
      "epoch: 635\n",
      "loss-----tensor(0.1493, grad_fn=<MulBackward0>)\n",
      "epoch: 636\n",
      "loss-----tensor(0.1468, grad_fn=<MulBackward0>)\n",
      "epoch: 637\n",
      "loss-----tensor(0.1714, grad_fn=<MulBackward0>)\n",
      "epoch: 638\n",
      "loss-----tensor(0.1710, grad_fn=<MulBackward0>)\n",
      "epoch: 639\n",
      "loss-----tensor(0.1495, grad_fn=<MulBackward0>)\n",
      "epoch: 640\n",
      "loss-----tensor(0.1470, grad_fn=<MulBackward0>)\n",
      "epoch: 641\n",
      "loss-----tensor(0.1419, grad_fn=<MulBackward0>)\n",
      "epoch: 642\n",
      "loss-----tensor(0.1699, grad_fn=<MulBackward0>)\n",
      "epoch: 643\n",
      "loss-----tensor(0.1539, grad_fn=<MulBackward0>)\n",
      "epoch: 644\n",
      "loss-----tensor(0.1512, grad_fn=<MulBackward0>)\n",
      "epoch: 645\n",
      "loss-----tensor(0.1465, grad_fn=<MulBackward0>)\n",
      "epoch: 646\n",
      "loss-----tensor(0.1448, grad_fn=<MulBackward0>)\n",
      "epoch: 647\n",
      "loss-----tensor(0.1396, grad_fn=<MulBackward0>)\n",
      "epoch: 648\n",
      "loss-----tensor(0.1600, grad_fn=<MulBackward0>)\n",
      "epoch: 649\n",
      "loss-----tensor(0.1517, grad_fn=<MulBackward0>)\n",
      "epoch: 650\n",
      "loss-----tensor(0.1871, grad_fn=<MulBackward0>)\n",
      "epoch: 651\n",
      "loss-----tensor(0.1417, grad_fn=<MulBackward0>)\n",
      "epoch: 652\n",
      "loss-----tensor(0.1475, grad_fn=<MulBackward0>)\n",
      "epoch: 653\n",
      "loss-----tensor(0.1545, grad_fn=<MulBackward0>)\n",
      "epoch: 654\n",
      "loss-----tensor(0.1431, grad_fn=<MulBackward0>)\n",
      "epoch: 655\n",
      "loss-----tensor(0.1444, grad_fn=<MulBackward0>)\n",
      "epoch: 656\n",
      "loss-----tensor(0.1410, grad_fn=<MulBackward0>)\n",
      "epoch: 657\n",
      "loss-----tensor(0.1365, grad_fn=<MulBackward0>)\n",
      "epoch: 658\n",
      "loss-----tensor(0.1434, grad_fn=<MulBackward0>)\n",
      "epoch: 659\n",
      "loss-----tensor(0.1422, grad_fn=<MulBackward0>)\n",
      "epoch: 660\n",
      "loss-----tensor(0.1422, grad_fn=<MulBackward0>)\n",
      "epoch: 661\n",
      "loss-----tensor(0.1476, grad_fn=<MulBackward0>)\n",
      "epoch: 662\n",
      "loss-----tensor(0.1407, grad_fn=<MulBackward0>)\n",
      "epoch: 663\n",
      "loss-----tensor(0.1413, grad_fn=<MulBackward0>)\n",
      "epoch: 664\n",
      "loss-----tensor(0.1424, grad_fn=<MulBackward0>)\n",
      "epoch: 665\n",
      "loss-----tensor(0.1377, grad_fn=<MulBackward0>)\n",
      "epoch: 666\n",
      "loss-----tensor(0.1674, grad_fn=<MulBackward0>)\n",
      "epoch: 667\n",
      "loss-----tensor(0.1365, grad_fn=<MulBackward0>)\n",
      "epoch: 668\n",
      "loss-----tensor(0.1500, grad_fn=<MulBackward0>)\n",
      "epoch: 669\n",
      "loss-----tensor(0.1336, grad_fn=<MulBackward0>)\n",
      "epoch: 670\n",
      "loss-----tensor(0.1434, grad_fn=<MulBackward0>)\n",
      "epoch: 671\n",
      "loss-----tensor(0.1427, grad_fn=<MulBackward0>)\n",
      "epoch: 672\n",
      "loss-----tensor(0.1323, grad_fn=<MulBackward0>)\n",
      "epoch: 673\n",
      "loss-----tensor(0.1442, grad_fn=<MulBackward0>)\n",
      "epoch: 674\n",
      "loss-----tensor(0.1394, grad_fn=<MulBackward0>)\n",
      "epoch: 675\n",
      "loss-----tensor(0.1331, grad_fn=<MulBackward0>)\n",
      "epoch: 676\n",
      "loss-----tensor(0.1482, grad_fn=<MulBackward0>)\n",
      "epoch: 677\n",
      "loss-----tensor(0.1338, grad_fn=<MulBackward0>)\n",
      "epoch: 678\n",
      "loss-----tensor(0.1432, grad_fn=<MulBackward0>)\n",
      "epoch: 679\n",
      "loss-----tensor(0.1395, grad_fn=<MulBackward0>)\n",
      "epoch: 680\n",
      "loss-----tensor(0.1366, grad_fn=<MulBackward0>)\n",
      "epoch: 681\n",
      "loss-----tensor(0.1327, grad_fn=<MulBackward0>)\n",
      "epoch: 682\n",
      "loss-----tensor(0.1349, grad_fn=<MulBackward0>)\n",
      "epoch: 683\n",
      "loss-----tensor(0.1282, grad_fn=<MulBackward0>)\n",
      "epoch: 684\n",
      "loss-----tensor(0.1324, grad_fn=<MulBackward0>)\n",
      "epoch: 685\n",
      "loss-----tensor(0.1366, grad_fn=<MulBackward0>)\n",
      "epoch: 686\n",
      "loss-----tensor(0.1345, grad_fn=<MulBackward0>)\n",
      "epoch: 687\n",
      "loss-----tensor(0.1355, grad_fn=<MulBackward0>)\n",
      "epoch: 688\n",
      "loss-----tensor(0.1364, grad_fn=<MulBackward0>)\n",
      "epoch: 689\n",
      "loss-----tensor(0.1307, grad_fn=<MulBackward0>)\n",
      "epoch: 690\n",
      "loss-----tensor(0.1323, grad_fn=<MulBackward0>)\n",
      "epoch: 691\n",
      "loss-----tensor(0.1741, grad_fn=<MulBackward0>)\n",
      "epoch: 692\n",
      "loss-----tensor(0.1295, grad_fn=<MulBackward0>)\n",
      "epoch: 693\n",
      "loss-----tensor(0.1306, grad_fn=<MulBackward0>)\n",
      "epoch: 694\n",
      "loss-----tensor(0.1397, grad_fn=<MulBackward0>)\n",
      "epoch: 695\n",
      "loss-----tensor(0.1257, grad_fn=<MulBackward0>)\n",
      "epoch: 696\n",
      "loss-----tensor(0.1300, grad_fn=<MulBackward0>)\n",
      "epoch: 697\n",
      "loss-----tensor(0.1295, grad_fn=<MulBackward0>)\n",
      "epoch: 698\n",
      "loss-----tensor(0.1380, grad_fn=<MulBackward0>)\n",
      "epoch: 699\n",
      "loss-----tensor(0.1280, grad_fn=<MulBackward0>)\n",
      "epoch: 700\n",
      "loss-----tensor(0.1257, grad_fn=<MulBackward0>)\n",
      "epoch: 701\n",
      "loss-----tensor(0.1273, grad_fn=<MulBackward0>)\n",
      "epoch: 702\n",
      "loss-----tensor(0.1309, grad_fn=<MulBackward0>)\n",
      "epoch: 703\n",
      "loss-----tensor(0.1253, grad_fn=<MulBackward0>)\n",
      "epoch: 704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss-----tensor(0.1676, grad_fn=<MulBackward0>)\n",
      "epoch: 705\n",
      "loss-----tensor(0.1278, grad_fn=<MulBackward0>)\n",
      "epoch: 706\n",
      "loss-----tensor(0.1250, grad_fn=<MulBackward0>)\n",
      "epoch: 707\n",
      "loss-----tensor(0.1279, grad_fn=<MulBackward0>)\n",
      "epoch: 708\n",
      "loss-----tensor(0.1344, grad_fn=<MulBackward0>)\n",
      "epoch: 709\n",
      "loss-----tensor(0.1758, grad_fn=<MulBackward0>)\n",
      "epoch: 710\n",
      "loss-----tensor(0.1298, grad_fn=<MulBackward0>)\n",
      "epoch: 711\n",
      "loss-----tensor(0.1310, grad_fn=<MulBackward0>)\n",
      "epoch: 712\n",
      "loss-----tensor(0.1287, grad_fn=<MulBackward0>)\n",
      "epoch: 713\n",
      "loss-----tensor(0.1222, grad_fn=<MulBackward0>)\n",
      "epoch: 714\n",
      "loss-----tensor(0.1258, grad_fn=<MulBackward0>)\n",
      "epoch: 715\n",
      "loss-----tensor(0.1258, grad_fn=<MulBackward0>)\n",
      "epoch: 716\n",
      "loss-----tensor(0.1248, grad_fn=<MulBackward0>)\n",
      "epoch: 717\n",
      "loss-----tensor(0.1268, grad_fn=<MulBackward0>)\n",
      "epoch: 718\n",
      "loss-----tensor(0.1311, grad_fn=<MulBackward0>)\n",
      "epoch: 719\n",
      "loss-----tensor(0.1300, grad_fn=<MulBackward0>)\n",
      "epoch: 720\n",
      "loss-----tensor(0.1251, grad_fn=<MulBackward0>)\n",
      "epoch: 721\n",
      "loss-----tensor(0.1219, grad_fn=<MulBackward0>)\n",
      "epoch: 722\n",
      "loss-----tensor(0.1236, grad_fn=<MulBackward0>)\n",
      "epoch: 723\n",
      "loss-----tensor(0.1257, grad_fn=<MulBackward0>)\n",
      "epoch: 724\n",
      "loss-----tensor(0.1226, grad_fn=<MulBackward0>)\n",
      "epoch: 725\n",
      "loss-----tensor(0.1258, grad_fn=<MulBackward0>)\n",
      "epoch: 726\n",
      "loss-----tensor(0.1283, grad_fn=<MulBackward0>)\n",
      "epoch: 727\n",
      "loss-----tensor(0.1382, grad_fn=<MulBackward0>)\n",
      "epoch: 728\n",
      "loss-----tensor(0.1244, grad_fn=<MulBackward0>)\n",
      "epoch: 729\n",
      "loss-----tensor(0.1227, grad_fn=<MulBackward0>)\n",
      "epoch: 730\n",
      "loss-----tensor(0.1237, grad_fn=<MulBackward0>)\n",
      "epoch: 731\n",
      "loss-----tensor(0.1275, grad_fn=<MulBackward0>)\n",
      "epoch: 732\n",
      "loss-----tensor(0.1213, grad_fn=<MulBackward0>)\n",
      "epoch: 733\n",
      "loss-----tensor(0.1222, grad_fn=<MulBackward0>)\n",
      "epoch: 734\n",
      "loss-----tensor(0.1266, grad_fn=<MulBackward0>)\n",
      "epoch: 735\n",
      "loss-----tensor(0.1213, grad_fn=<MulBackward0>)\n",
      "epoch: 736\n",
      "loss-----tensor(0.1195, grad_fn=<MulBackward0>)\n",
      "epoch: 737\n",
      "loss-----tensor(0.1283, grad_fn=<MulBackward0>)\n",
      "epoch: 738\n",
      "loss-----tensor(0.1194, grad_fn=<MulBackward0>)\n",
      "epoch: 739\n",
      "loss-----tensor(0.1132, grad_fn=<MulBackward0>)\n",
      "epoch: 740\n",
      "loss-----tensor(0.1261, grad_fn=<MulBackward0>)\n",
      "epoch: 741\n",
      "loss-----tensor(0.1232, grad_fn=<MulBackward0>)\n",
      "epoch: 742\n",
      "loss-----tensor(0.1172, grad_fn=<MulBackward0>)\n",
      "epoch: 743\n",
      "loss-----tensor(0.1145, grad_fn=<MulBackward0>)\n",
      "epoch: 744\n",
      "loss-----tensor(0.1189, grad_fn=<MulBackward0>)\n",
      "epoch: 745\n",
      "loss-----tensor(0.1202, grad_fn=<MulBackward0>)\n",
      "epoch: 746\n",
      "loss-----tensor(0.1177, grad_fn=<MulBackward0>)\n",
      "epoch: 747\n",
      "loss-----tensor(0.1246, grad_fn=<MulBackward0>)\n",
      "epoch: 748\n",
      "loss-----tensor(0.1168, grad_fn=<MulBackward0>)\n",
      "epoch: 749\n",
      "loss-----tensor(0.1151, grad_fn=<MulBackward0>)\n",
      "epoch: 750\n",
      "loss-----tensor(0.1180, grad_fn=<MulBackward0>)\n",
      "epoch: 751\n",
      "loss-----tensor(0.1199, grad_fn=<MulBackward0>)\n",
      "epoch: 752\n",
      "loss-----tensor(0.1187, grad_fn=<MulBackward0>)\n",
      "epoch: 753\n",
      "loss-----tensor(0.1175, grad_fn=<MulBackward0>)\n",
      "epoch: 754\n",
      "loss-----tensor(0.1149, grad_fn=<MulBackward0>)\n",
      "epoch: 755\n",
      "loss-----tensor(0.1118, grad_fn=<MulBackward0>)\n",
      "epoch: 756\n",
      "loss-----tensor(0.1207, grad_fn=<MulBackward0>)\n",
      "epoch: 757\n",
      "loss-----tensor(0.1215, grad_fn=<MulBackward0>)\n",
      "epoch: 758\n",
      "loss-----tensor(0.1193, grad_fn=<MulBackward0>)\n",
      "epoch: 759\n",
      "loss-----tensor(0.1166, grad_fn=<MulBackward0>)\n",
      "epoch: 760\n",
      "loss-----tensor(0.1108, grad_fn=<MulBackward0>)\n",
      "epoch: 761\n",
      "loss-----tensor(0.1184, grad_fn=<MulBackward0>)\n",
      "epoch: 762\n",
      "loss-----tensor(0.1185, grad_fn=<MulBackward0>)\n",
      "epoch: 763\n",
      "loss-----tensor(0.1375, grad_fn=<MulBackward0>)\n",
      "epoch: 764\n",
      "loss-----tensor(0.1097, grad_fn=<MulBackward0>)\n",
      "epoch: 765\n",
      "loss-----tensor(0.1172, grad_fn=<MulBackward0>)\n",
      "epoch: 766\n",
      "loss-----tensor(0.1195, grad_fn=<MulBackward0>)\n",
      "epoch: 767\n",
      "loss-----tensor(0.1355, grad_fn=<MulBackward0>)\n",
      "epoch: 768\n",
      "loss-----tensor(0.1163, grad_fn=<MulBackward0>)\n",
      "epoch: 769\n",
      "loss-----tensor(0.1120, grad_fn=<MulBackward0>)\n",
      "epoch: 770\n",
      "loss-----tensor(0.1153, grad_fn=<MulBackward0>)\n",
      "epoch: 771\n",
      "loss-----tensor(0.1843, grad_fn=<MulBackward0>)\n",
      "epoch: 772\n",
      "loss-----tensor(0.1114, grad_fn=<MulBackward0>)\n",
      "epoch: 773\n",
      "loss-----tensor(0.1176, grad_fn=<MulBackward0>)\n",
      "epoch: 774\n",
      "loss-----tensor(0.1156, grad_fn=<MulBackward0>)\n",
      "epoch: 775\n",
      "loss-----tensor(0.1101, grad_fn=<MulBackward0>)\n",
      "epoch: 776\n",
      "loss-----tensor(0.1091, grad_fn=<MulBackward0>)\n",
      "epoch: 777\n",
      "loss-----tensor(0.1113, grad_fn=<MulBackward0>)\n",
      "epoch: 778\n",
      "loss-----tensor(0.1144, grad_fn=<MulBackward0>)\n",
      "epoch: 779\n",
      "loss-----tensor(0.1103, grad_fn=<MulBackward0>)\n",
      "epoch: 780\n",
      "loss-----tensor(0.1095, grad_fn=<MulBackward0>)\n",
      "epoch: 781\n",
      "loss-----tensor(0.1104, grad_fn=<MulBackward0>)\n",
      "epoch: 782\n",
      "loss-----tensor(0.1354, grad_fn=<MulBackward0>)\n",
      "epoch: 783\n",
      "loss-----tensor(0.1069, grad_fn=<MulBackward0>)\n",
      "epoch: 784\n",
      "loss-----tensor(0.1065, grad_fn=<MulBackward0>)\n",
      "epoch: 785\n",
      "loss-----tensor(0.1168, grad_fn=<MulBackward0>)\n",
      "epoch: 786\n",
      "loss-----tensor(0.1099, grad_fn=<MulBackward0>)\n",
      "epoch: 787\n",
      "loss-----tensor(0.1130, grad_fn=<MulBackward0>)\n",
      "epoch: 788\n",
      "loss-----tensor(0.1114, grad_fn=<MulBackward0>)\n",
      "epoch: 789\n",
      "loss-----tensor(0.1040, grad_fn=<MulBackward0>)\n",
      "epoch: 790\n",
      "loss-----tensor(0.1054, grad_fn=<MulBackward0>)\n",
      "epoch: 791\n",
      "loss-----tensor(0.1102, grad_fn=<MulBackward0>)\n",
      "epoch: 792\n",
      "loss-----tensor(0.1088, grad_fn=<MulBackward0>)\n",
      "epoch: 793\n",
      "loss-----tensor(0.1045, grad_fn=<MulBackward0>)\n",
      "epoch: 794\n",
      "loss-----tensor(0.1077, grad_fn=<MulBackward0>)\n",
      "epoch: 795\n",
      "loss-----tensor(0.1141, grad_fn=<MulBackward0>)\n",
      "epoch: 796\n",
      "loss-----tensor(0.1170, grad_fn=<MulBackward0>)\n",
      "epoch: 797\n",
      "loss-----tensor(0.1025, grad_fn=<MulBackward0>)\n",
      "epoch: 798\n",
      "loss-----tensor(0.1158, grad_fn=<MulBackward0>)\n",
      "epoch: 799\n",
      "loss-----tensor(0.1226, grad_fn=<MulBackward0>)\n",
      "epoch: 800\n",
      "loss-----tensor(0.1100, grad_fn=<MulBackward0>)\n",
      "epoch: 801\n",
      "loss-----tensor(0.1123, grad_fn=<MulBackward0>)\n",
      "epoch: 802\n",
      "loss-----tensor(0.1100, grad_fn=<MulBackward0>)\n",
      "epoch: 803\n",
      "loss-----tensor(0.1040, grad_fn=<MulBackward0>)\n",
      "epoch: 804\n",
      "loss-----tensor(0.1045, grad_fn=<MulBackward0>)\n",
      "epoch: 805\n",
      "loss-----tensor(0.1038, grad_fn=<MulBackward0>)\n",
      "epoch: 806\n",
      "loss-----tensor(0.1067, grad_fn=<MulBackward0>)\n",
      "epoch: 807\n",
      "loss-----tensor(0.1039, grad_fn=<MulBackward0>)\n",
      "epoch: 808\n",
      "loss-----tensor(0.1055, grad_fn=<MulBackward0>)\n",
      "epoch: 809\n",
      "loss-----tensor(0.1013, grad_fn=<MulBackward0>)\n",
      "epoch: 810\n",
      "loss-----tensor(0.1042, grad_fn=<MulBackward0>)\n",
      "epoch: 811\n",
      "loss-----tensor(0.1028, grad_fn=<MulBackward0>)\n",
      "epoch: 812\n",
      "loss-----tensor(0.1314, grad_fn=<MulBackward0>)\n",
      "epoch: 813\n",
      "loss-----tensor(0.1127, grad_fn=<MulBackward0>)\n",
      "epoch: 814\n",
      "loss-----tensor(0.1052, grad_fn=<MulBackward0>)\n",
      "epoch: 815\n",
      "loss-----tensor(0.1044, grad_fn=<MulBackward0>)\n",
      "epoch: 816\n",
      "loss-----tensor(0.1006, grad_fn=<MulBackward0>)\n",
      "epoch: 817\n",
      "loss-----tensor(0.1079, grad_fn=<MulBackward0>)\n",
      "epoch: 818\n",
      "loss-----tensor(0.1004, grad_fn=<MulBackward0>)\n",
      "epoch: 819\n",
      "loss-----tensor(0.1031, grad_fn=<MulBackward0>)\n",
      "epoch: 820\n",
      "loss-----tensor(0.1101, grad_fn=<MulBackward0>)\n",
      "epoch: 821\n",
      "loss-----tensor(0.1079, grad_fn=<MulBackward0>)\n",
      "epoch: 822\n",
      "loss-----tensor(0.1024, grad_fn=<MulBackward0>)\n",
      "epoch: 823\n",
      "loss-----tensor(0.1174, grad_fn=<MulBackward0>)\n",
      "epoch: 824\n",
      "loss-----tensor(0.1073, grad_fn=<MulBackward0>)\n",
      "epoch: 825\n",
      "loss-----tensor(0.0958, grad_fn=<MulBackward0>)\n",
      "epoch: 826\n",
      "loss-----tensor(0.0986, grad_fn=<MulBackward0>)\n",
      "epoch: 827\n",
      "loss-----tensor(0.1021, grad_fn=<MulBackward0>)\n",
      "epoch: 828\n",
      "loss-----tensor(0.1013, grad_fn=<MulBackward0>)\n",
      "epoch: 829\n",
      "loss-----tensor(0.1441, grad_fn=<MulBackward0>)\n",
      "epoch: 830\n",
      "loss-----tensor(0.1012, grad_fn=<MulBackward0>)\n",
      "epoch: 831\n",
      "loss-----tensor(0.1008, grad_fn=<MulBackward0>)\n",
      "epoch: 832\n",
      "loss-----tensor(0.1014, grad_fn=<MulBackward0>)\n",
      "epoch: 833\n",
      "loss-----tensor(0.1042, grad_fn=<MulBackward0>)\n",
      "epoch: 834\n",
      "loss-----tensor(0.1313, grad_fn=<MulBackward0>)\n",
      "epoch: 835\n",
      "loss-----tensor(0.0957, grad_fn=<MulBackward0>)\n",
      "epoch: 836\n",
      "loss-----tensor(0.1004, grad_fn=<MulBackward0>)\n",
      "epoch: 837\n",
      "loss-----tensor(0.1005, grad_fn=<MulBackward0>)\n",
      "epoch: 838\n",
      "loss-----tensor(0.1003, grad_fn=<MulBackward0>)\n",
      "epoch: 839\n",
      "loss-----tensor(0.0997, grad_fn=<MulBackward0>)\n",
      "epoch: 840\n",
      "loss-----tensor(0.1017, grad_fn=<MulBackward0>)\n",
      "epoch: 841\n",
      "loss-----tensor(0.1037, grad_fn=<MulBackward0>)\n",
      "epoch: 842\n",
      "loss-----tensor(0.1024, grad_fn=<MulBackward0>)\n",
      "epoch: 843\n",
      "loss-----tensor(0.1078, grad_fn=<MulBackward0>)\n",
      "epoch: 844\n",
      "loss-----tensor(0.0967, grad_fn=<MulBackward0>)\n",
      "epoch: 845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss-----tensor(0.0953, grad_fn=<MulBackward0>)\n",
      "epoch: 846\n",
      "loss-----tensor(0.1003, grad_fn=<MulBackward0>)\n",
      "epoch: 847\n",
      "loss-----tensor(0.0960, grad_fn=<MulBackward0>)\n",
      "epoch: 848\n",
      "loss-----tensor(0.1278, grad_fn=<MulBackward0>)\n",
      "epoch: 849\n",
      "loss-----tensor(0.0986, grad_fn=<MulBackward0>)\n",
      "epoch: 850\n",
      "loss-----tensor(0.0994, grad_fn=<MulBackward0>)\n",
      "epoch: 851\n",
      "loss-----tensor(0.1020, grad_fn=<MulBackward0>)\n",
      "epoch: 852\n",
      "loss-----tensor(0.0968, grad_fn=<MulBackward0>)\n",
      "epoch: 853\n",
      "loss-----tensor(0.0936, grad_fn=<MulBackward0>)\n",
      "epoch: 854\n",
      "loss-----tensor(0.0936, grad_fn=<MulBackward0>)\n",
      "epoch: 855\n",
      "loss-----tensor(0.1001, grad_fn=<MulBackward0>)\n",
      "epoch: 856\n",
      "loss-----tensor(0.0950, grad_fn=<MulBackward0>)\n",
      "epoch: 857\n",
      "loss-----tensor(0.0935, grad_fn=<MulBackward0>)\n",
      "epoch: 858\n",
      "loss-----tensor(0.0967, grad_fn=<MulBackward0>)\n",
      "epoch: 859\n",
      "loss-----tensor(0.0946, grad_fn=<MulBackward0>)\n",
      "epoch: 860\n",
      "loss-----tensor(0.0966, grad_fn=<MulBackward0>)\n",
      "epoch: 861\n",
      "loss-----tensor(0.0962, grad_fn=<MulBackward0>)\n",
      "epoch: 862\n",
      "loss-----tensor(0.0960, grad_fn=<MulBackward0>)\n",
      "epoch: 863\n",
      "loss-----tensor(0.0941, grad_fn=<MulBackward0>)\n",
      "epoch: 864\n",
      "loss-----tensor(0.0936, grad_fn=<MulBackward0>)\n",
      "epoch: 865\n",
      "loss-----tensor(0.0950, grad_fn=<MulBackward0>)\n",
      "epoch: 866\n",
      "loss-----tensor(0.0951, grad_fn=<MulBackward0>)\n",
      "epoch: 867\n",
      "loss-----tensor(0.0932, grad_fn=<MulBackward0>)\n",
      "epoch: 868\n",
      "loss-----tensor(0.0930, grad_fn=<MulBackward0>)\n",
      "epoch: 869\n",
      "loss-----tensor(0.0915, grad_fn=<MulBackward0>)\n",
      "epoch: 870\n",
      "loss-----tensor(0.0967, grad_fn=<MulBackward0>)\n",
      "epoch: 871\n",
      "loss-----tensor(0.1131, grad_fn=<MulBackward0>)\n",
      "epoch: 872\n",
      "loss-----tensor(0.1027, grad_fn=<MulBackward0>)\n",
      "epoch: 873\n",
      "loss-----tensor(0.1006, grad_fn=<MulBackward0>)\n",
      "epoch: 874\n",
      "loss-----tensor(0.0916, grad_fn=<MulBackward0>)\n",
      "epoch: 875\n",
      "loss-----tensor(0.0911, grad_fn=<MulBackward0>)\n",
      "epoch: 876\n",
      "loss-----tensor(0.0926, grad_fn=<MulBackward0>)\n",
      "epoch: 877\n",
      "loss-----tensor(0.0902, grad_fn=<MulBackward0>)\n",
      "epoch: 878\n",
      "loss-----tensor(0.0924, grad_fn=<MulBackward0>)\n",
      "epoch: 879\n",
      "loss-----tensor(0.0980, grad_fn=<MulBackward0>)\n",
      "epoch: 880\n",
      "loss-----tensor(0.0913, grad_fn=<MulBackward0>)\n",
      "epoch: 881\n",
      "loss-----tensor(0.0916, grad_fn=<MulBackward0>)\n",
      "epoch: 882\n",
      "loss-----tensor(0.0927, grad_fn=<MulBackward0>)\n",
      "epoch: 883\n",
      "loss-----tensor(0.0920, grad_fn=<MulBackward0>)\n",
      "epoch: 884\n",
      "loss-----tensor(0.0943, grad_fn=<MulBackward0>)\n",
      "epoch: 885\n",
      "loss-----tensor(0.0899, grad_fn=<MulBackward0>)\n",
      "epoch: 886\n",
      "loss-----tensor(0.1001, grad_fn=<MulBackward0>)\n",
      "epoch: 887\n",
      "loss-----tensor(0.1016, grad_fn=<MulBackward0>)\n",
      "epoch: 888\n",
      "loss-----tensor(0.0866, grad_fn=<MulBackward0>)\n",
      "epoch: 889\n",
      "loss-----tensor(0.0967, grad_fn=<MulBackward0>)\n",
      "epoch: 890\n",
      "loss-----tensor(0.0920, grad_fn=<MulBackward0>)\n",
      "epoch: 891\n",
      "loss-----tensor(0.0881, grad_fn=<MulBackward0>)\n",
      "epoch: 892\n",
      "loss-----tensor(0.0938, grad_fn=<MulBackward0>)\n",
      "epoch: 893\n",
      "loss-----tensor(0.0930, grad_fn=<MulBackward0>)\n",
      "epoch: 894\n",
      "loss-----tensor(0.0875, grad_fn=<MulBackward0>)\n",
      "epoch: 895\n",
      "loss-----tensor(0.0949, grad_fn=<MulBackward0>)\n",
      "epoch: 896\n",
      "loss-----tensor(0.0978, grad_fn=<MulBackward0>)\n",
      "epoch: 897\n",
      "loss-----tensor(0.0901, grad_fn=<MulBackward0>)\n",
      "epoch: 898\n",
      "loss-----tensor(0.0868, grad_fn=<MulBackward0>)\n",
      "epoch: 899\n",
      "loss-----tensor(0.0941, grad_fn=<MulBackward0>)\n",
      "epoch: 900\n",
      "loss-----tensor(0.0864, grad_fn=<MulBackward0>)\n",
      "epoch: 901\n",
      "loss-----tensor(0.0883, grad_fn=<MulBackward0>)\n",
      "epoch: 902\n",
      "loss-----tensor(0.0909, grad_fn=<MulBackward0>)\n",
      "epoch: 903\n",
      "loss-----tensor(0.0933, grad_fn=<MulBackward0>)\n",
      "epoch: 904\n",
      "loss-----tensor(0.0860, grad_fn=<MulBackward0>)\n",
      "epoch: 905\n",
      "loss-----tensor(0.0953, grad_fn=<MulBackward0>)\n",
      "epoch: 906\n",
      "loss-----tensor(0.0882, grad_fn=<MulBackward0>)\n",
      "epoch: 907\n",
      "loss-----tensor(0.0866, grad_fn=<MulBackward0>)\n",
      "epoch: 908\n",
      "loss-----tensor(0.0926, grad_fn=<MulBackward0>)\n",
      "epoch: 909\n",
      "loss-----tensor(0.0883, grad_fn=<MulBackward0>)\n",
      "epoch: 910\n",
      "loss-----tensor(0.0904, grad_fn=<MulBackward0>)\n",
      "epoch: 911\n",
      "loss-----tensor(0.0869, grad_fn=<MulBackward0>)\n",
      "epoch: 912\n",
      "loss-----tensor(0.0889, grad_fn=<MulBackward0>)\n",
      "epoch: 913\n",
      "loss-----tensor(0.0936, grad_fn=<MulBackward0>)\n",
      "epoch: 914\n",
      "loss-----tensor(0.0881, grad_fn=<MulBackward0>)\n",
      "epoch: 915\n",
      "loss-----tensor(0.0865, grad_fn=<MulBackward0>)\n",
      "epoch: 916\n",
      "loss-----tensor(0.1022, grad_fn=<MulBackward0>)\n",
      "epoch: 917\n",
      "loss-----tensor(0.0878, grad_fn=<MulBackward0>)\n",
      "epoch: 918\n",
      "loss-----tensor(0.0870, grad_fn=<MulBackward0>)\n",
      "epoch: 919\n",
      "loss-----tensor(0.0849, grad_fn=<MulBackward0>)\n",
      "epoch: 920\n",
      "loss-----tensor(0.0924, grad_fn=<MulBackward0>)\n",
      "epoch: 921\n",
      "loss-----tensor(0.0857, grad_fn=<MulBackward0>)\n",
      "epoch: 922\n",
      "loss-----tensor(0.0921, grad_fn=<MulBackward0>)\n",
      "epoch: 923\n",
      "loss-----tensor(0.0880, grad_fn=<MulBackward0>)\n",
      "epoch: 924\n",
      "loss-----tensor(0.0852, grad_fn=<MulBackward0>)\n",
      "epoch: 925\n",
      "loss-----tensor(0.0842, grad_fn=<MulBackward0>)\n",
      "epoch: 926\n",
      "loss-----tensor(0.0829, grad_fn=<MulBackward0>)\n",
      "epoch: 927\n",
      "loss-----tensor(0.0858, grad_fn=<MulBackward0>)\n",
      "epoch: 928\n",
      "loss-----tensor(0.0829, grad_fn=<MulBackward0>)\n",
      "epoch: 929\n",
      "loss-----tensor(0.0855, grad_fn=<MulBackward0>)\n",
      "epoch: 930\n",
      "loss-----tensor(0.0902, grad_fn=<MulBackward0>)\n",
      "epoch: 931\n",
      "loss-----tensor(0.0833, grad_fn=<MulBackward0>)\n",
      "epoch: 932\n",
      "loss-----tensor(0.0833, grad_fn=<MulBackward0>)\n",
      "epoch: 933\n",
      "loss-----tensor(0.0840, grad_fn=<MulBackward0>)\n",
      "epoch: 934\n",
      "loss-----tensor(0.0840, grad_fn=<MulBackward0>)\n",
      "epoch: 935\n",
      "loss-----tensor(0.0856, grad_fn=<MulBackward0>)\n",
      "epoch: 936\n",
      "loss-----tensor(0.0851, grad_fn=<MulBackward0>)\n",
      "epoch: 937\n",
      "loss-----tensor(0.0816, grad_fn=<MulBackward0>)\n",
      "epoch: 938\n",
      "loss-----tensor(0.0873, grad_fn=<MulBackward0>)\n",
      "epoch: 939\n",
      "loss-----tensor(0.0871, grad_fn=<MulBackward0>)\n",
      "epoch: 940\n",
      "loss-----tensor(0.0840, grad_fn=<MulBackward0>)\n",
      "epoch: 941\n",
      "loss-----tensor(0.0887, grad_fn=<MulBackward0>)\n",
      "epoch: 942\n",
      "loss-----tensor(0.0971, grad_fn=<MulBackward0>)\n",
      "epoch: 943\n",
      "loss-----tensor(0.0851, grad_fn=<MulBackward0>)\n",
      "epoch: 944\n",
      "loss-----tensor(0.0826, grad_fn=<MulBackward0>)\n",
      "epoch: 945\n",
      "loss-----tensor(0.0867, grad_fn=<MulBackward0>)\n",
      "epoch: 946\n",
      "loss-----tensor(0.0811, grad_fn=<MulBackward0>)\n",
      "epoch: 947\n",
      "loss-----tensor(0.0848, grad_fn=<MulBackward0>)\n",
      "epoch: 948\n",
      "loss-----tensor(0.0812, grad_fn=<MulBackward0>)\n",
      "epoch: 949\n",
      "loss-----tensor(0.0795, grad_fn=<MulBackward0>)\n",
      "epoch: 950\n",
      "loss-----tensor(0.0890, grad_fn=<MulBackward0>)\n",
      "epoch: 951\n",
      "loss-----tensor(0.0911, grad_fn=<MulBackward0>)\n",
      "epoch: 952\n",
      "loss-----tensor(0.0834, grad_fn=<MulBackward0>)\n",
      "epoch: 953\n",
      "loss-----tensor(0.0786, grad_fn=<MulBackward0>)\n",
      "epoch: 954\n",
      "loss-----tensor(0.0836, grad_fn=<MulBackward0>)\n",
      "epoch: 955\n",
      "loss-----tensor(0.0829, grad_fn=<MulBackward0>)\n",
      "epoch: 956\n",
      "loss-----tensor(0.0805, grad_fn=<MulBackward0>)\n",
      "epoch: 957\n",
      "loss-----tensor(0.0810, grad_fn=<MulBackward0>)\n",
      "epoch: 958\n",
      "loss-----tensor(0.0791, grad_fn=<MulBackward0>)\n",
      "epoch: 959\n",
      "loss-----tensor(0.0868, grad_fn=<MulBackward0>)\n",
      "epoch: 960\n",
      "loss-----tensor(0.0914, grad_fn=<MulBackward0>)\n",
      "epoch: 961\n",
      "loss-----tensor(0.0774, grad_fn=<MulBackward0>)\n",
      "epoch: 962\n",
      "loss-----tensor(0.0817, grad_fn=<MulBackward0>)\n",
      "epoch: 963\n",
      "loss-----tensor(0.0874, grad_fn=<MulBackward0>)\n",
      "epoch: 964\n",
      "loss-----tensor(0.0802, grad_fn=<MulBackward0>)\n",
      "epoch: 965\n",
      "loss-----tensor(0.0801, grad_fn=<MulBackward0>)\n",
      "epoch: 966\n",
      "loss-----tensor(0.0935, grad_fn=<MulBackward0>)\n",
      "epoch: 967\n",
      "loss-----tensor(0.0902, grad_fn=<MulBackward0>)\n",
      "epoch: 968\n",
      "loss-----tensor(0.0802, grad_fn=<MulBackward0>)\n",
      "epoch: 969\n",
      "loss-----tensor(0.0817, grad_fn=<MulBackward0>)\n",
      "epoch: 970\n",
      "loss-----tensor(0.0837, grad_fn=<MulBackward0>)\n",
      "epoch: 971\n",
      "loss-----tensor(0.0821, grad_fn=<MulBackward0>)\n",
      "epoch: 972\n",
      "loss-----tensor(0.0783, grad_fn=<MulBackward0>)\n",
      "epoch: 973\n",
      "loss-----tensor(0.0956, grad_fn=<MulBackward0>)\n",
      "epoch: 974\n",
      "loss-----tensor(0.0841, grad_fn=<MulBackward0>)\n",
      "epoch: 975\n",
      "loss-----tensor(0.0780, grad_fn=<MulBackward0>)\n",
      "epoch: 976\n",
      "loss-----tensor(0.0795, grad_fn=<MulBackward0>)\n",
      "epoch: 977\n",
      "loss-----tensor(0.0783, grad_fn=<MulBackward0>)\n",
      "epoch: 978\n",
      "loss-----tensor(0.0991, grad_fn=<MulBackward0>)\n",
      "epoch: 979\n",
      "loss-----tensor(0.0778, grad_fn=<MulBackward0>)\n",
      "epoch: 980\n",
      "loss-----tensor(0.0766, grad_fn=<MulBackward0>)\n",
      "epoch: 981\n",
      "loss-----tensor(0.0789, grad_fn=<MulBackward0>)\n",
      "epoch: 982\n",
      "loss-----tensor(0.0779, grad_fn=<MulBackward0>)\n",
      "epoch: 983\n",
      "loss-----tensor(0.0810, grad_fn=<MulBackward0>)\n",
      "epoch: 984\n",
      "loss-----tensor(0.0784, grad_fn=<MulBackward0>)\n",
      "epoch: 985\n",
      "loss-----tensor(0.0793, grad_fn=<MulBackward0>)\n",
      "epoch: 986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss-----tensor(0.0814, grad_fn=<MulBackward0>)\n",
      "epoch: 987\n",
      "loss-----tensor(0.0805, grad_fn=<MulBackward0>)\n",
      "epoch: 988\n",
      "loss-----tensor(0.0798, grad_fn=<MulBackward0>)\n",
      "epoch: 989\n",
      "loss-----tensor(0.0796, grad_fn=<MulBackward0>)\n",
      "epoch: 990\n",
      "loss-----tensor(0.0791, grad_fn=<MulBackward0>)\n",
      "epoch: 991\n",
      "loss-----tensor(0.0750, grad_fn=<MulBackward0>)\n",
      "epoch: 992\n",
      "loss-----tensor(0.0843, grad_fn=<MulBackward0>)\n",
      "epoch: 993\n",
      "loss-----tensor(0.0750, grad_fn=<MulBackward0>)\n",
      "epoch: 994\n",
      "loss-----tensor(0.0774, grad_fn=<MulBackward0>)\n",
      "epoch: 995\n",
      "loss-----tensor(0.0791, grad_fn=<MulBackward0>)\n",
      "epoch: 996\n",
      "loss-----tensor(0.0802, grad_fn=<MulBackward0>)\n",
      "epoch: 997\n",
      "loss-----tensor(0.0776, grad_fn=<MulBackward0>)\n",
      "epoch: 998\n",
      "loss-----tensor(0.0781, grad_fn=<MulBackward0>)\n",
      "epoch: 999\n",
      "loss-----tensor(0.0795, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Make sure prepare_sequence from earlier in the LSTM section is loaded（为什么？）\n",
    "for epoch in range(10):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    print(\"epoch: \"+str(epoch))\n",
    "    for sentence, tags in training_data:\n",
    "        # 1. 清空梯度\n",
    "        model.zero_grad()\n",
    "\n",
    "        # 2. 准备输入\n",
    "        sentence_in = torch.tensor([word_to_ix[t] for t in sentence], dtype=torch.long)  #prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "\n",
    "        # 3. 运行模型\n",
    "        loss = model(sentence_in, targets)\n",
    "        \n",
    "        # 4. 计算loss值，梯度并更新权重参数                                 \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"loss-----\"+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, sentence):\n",
    "        #从BiLSTM得到emission scores(也就是tagset_size长度的向量组)\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        \n",
    "        #给定lstm+全连接的特征值们，通过viterbi算法找到最优路径\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        \n",
    "        return score, tag_seq\n",
    "\n",
    "    def _viterbi_decode(self, feats):     #feats=(len(sentence)=11, batch_size=1, len(tag_to_ix)=5)=(11,1,5)\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
