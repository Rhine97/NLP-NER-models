{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 说明\n",
    "从文本内容分解开始的baseline数据\n",
    "\n",
    "环境：（lenovo）base  \n",
    "python = 3.7.6  \n",
    "pytorch = 1.4.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import jieba\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchcrf import CRF    #This class provides an implementation of a CRF layer.\n",
    "from torch.utils.data import Dataset,DataLoader, RandomSampler\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各参数\n",
    "trainPath = \"./dataset/train_data_public.csv\"\n",
    "dicPath = \"./dataset/vocab.txt\"\n",
    "log_path = './Logs/'\n",
    "model_path = './Models/'\n",
    "rq = time.strftime('%Y%m%d%H%M', time.localtime(time.time()))\n",
    "\n",
    "#创建路径\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一步，创建一个logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)  # Log等级总开关\n",
    "\n",
    "#创建一个handler，用于写入日志文件\n",
    "logfile = log_path + rq + '.log'   #log日志的文件名称\n",
    "fh = logging.FileHandler(logfile, mode='w')\n",
    "fh.setLevel(logging.DEBUG)  # 输出到file的log等级的开关\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)  # 输出到console的log等级的开关\n",
    "\n",
    "# 第三步，定义handler的输出格式\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s\")\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# 第四步，将logger添加到handler里面\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "# 日志\n",
    "#logger.debug('this is a logger debug message')\n",
    "#logger.info('this is a logger info message')\n",
    "#logger.warning('this is a logger warning message')\n",
    "#logger.error('this is a logger error message')\n",
    "#logger.critical('this is a logger critical message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 18:10:44,354 - <ipython-input-4-ee9c4350caf8>[line:4] - INFO: open train dataset,shape=(7528, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exist\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>BIO_anno</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...</td>\n",
       "      <td>B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>单标我有了，最近visa双标返现活动好</td>\n",
       "      <td>B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>建设银行提额很慢的……</td>\n",
       "      <td>B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k</td>\n",
       "      <td>O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>利率不错，可以撸</td>\n",
       "      <td>B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COM...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>7523</td>\n",
       "      <td>我鼎级拒了</td>\n",
       "      <td>O O O B-COMMENTS_ADJ O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>7524</td>\n",
       "      <td>一打一个准，准胜，看激活信用卡时那协议，全是对银行有利的</td>\n",
       "      <td>O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>7525</td>\n",
       "      <td>招行分期白80k</td>\n",
       "      <td>B-BANK I-BANK B-PRODUCT I-PRODUCT I-PRODUCT O O O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>7526</td>\n",
       "      <td>5万，额度还行吧没毕业哦</td>\n",
       "      <td>O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>7527</td>\n",
       "      <td>张家港农商、江阴农商、无锡农商试试</td>\n",
       "      <td>B-BANK I-BANK I-BANK I-BANK I-BANK O B-BANK I-...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7528 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0        0  交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...   \n",
       "1        1                                单标我有了，最近visa双标返现活动好   \n",
       "2        2                                        建设银行提额很慢的……   \n",
       "3        3                 我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k   \n",
       "4        4                                           利率不错，可以撸   \n",
       "...    ...                                                ...   \n",
       "7523  7523                                              我鼎级拒了   \n",
       "7524  7524                       一打一个准，准胜，看激活信用卡时那协议，全是对银行有利的   \n",
       "7525  7525                                           招行分期白80k   \n",
       "7526  7526                                       5万，额度还行吧没毕业哦   \n",
       "7527  7527                                  张家港农商、江阴农商、无锡农商试试   \n",
       "\n",
       "                                               BIO_anno  class  \n",
       "0     B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...      0  \n",
       "1     B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...      1  \n",
       "2     B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...      0  \n",
       "3     O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...      2  \n",
       "4     B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COM...      1  \n",
       "...                                                 ...    ...  \n",
       "7523                             O O O B-COMMENTS_ADJ O      2  \n",
       "7524  O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...      2  \n",
       "7525  B-BANK I-BANK B-PRODUCT I-PRODUCT I-PRODUCT O O O      2  \n",
       "7526      O O O B-COMMENTS_N I-COMMENTS_N O O O O O O O      2  \n",
       "7527  B-BANK I-BANK I-BANK I-BANK I-BANK O B-BANK I-...      2  \n",
       "\n",
       "[7528 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile(trainPath):\n",
    "    print(\"file exist\")\n",
    "    dataset = pd.read_csv(trainPath)\n",
    "logger.info('open train dataset,shape='+str(dataset.shape))\n",
    "dataset   #共10000条数据，属性分为：unnamed，text，BIO_anno，class，bank_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset['text']\n",
    "text[:10]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集输入合格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50    #句子的标准长度\n",
    "BATCH_SIZE = 8  #minibatch的大小\n",
    "EMBEDDING_DIM = 120\n",
    "HIDDEN_DIM = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 tag to index 词典\n",
    "def get_tag2index():\n",
    "    return {\"O\": 0,\n",
    "            \"B-BANK\":1,\"I-BANK\":2,         #银行实体\n",
    "            \"B-PRODUCT\":3,\"I-PRODUCT\":4,   #产品实体\n",
    "            \"B-COMMENTS_N\":5,\"I-COMMENTS_N\":6,   #用户评论，名词\n",
    "            \"B-COMMENTS_ADJ\":7,\"I-COMMENTS_ADJ\":8    #用户评论，形容词\n",
    "            }\n",
    "# 获取 word to index 词典\n",
    "def get_w2i(vocab_path = dicPath):\n",
    "    w2i = {}\n",
    "    with open(vocab_path, encoding = 'utf-8') as f:\n",
    "        while True:\n",
    "            text = f.readline()\n",
    "            if not text:\n",
    "                break\n",
    "            text = text.strip()\n",
    "            if text and len(text) > 0:\n",
    "                w2i[text] = len(w2i) + 1\n",
    "    return w2i\n",
    "\n",
    "def pad2mask(t):\n",
    "    if t==pad_index: #转换成mask所用的0\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def text_tag_to_index(dataset):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    for row in range(len(dataset)):\n",
    "        text = dataset.iloc[row]['text']\n",
    "        tag = dataset.iloc[row]['BIO_anno']\n",
    "        #text\n",
    "        #tag\n",
    "        if len(text)!=len(tag):  #如果从数据集获得的text和label长度不一致，就不要了？（那对验证会不会有影响）\n",
    "            next\n",
    "        #print(\"text长度：\"+str(len(text)))\n",
    "\n",
    "        #1. word转index\n",
    "        #1.1 text词汇\n",
    "        text_index = []\n",
    "        text_index.append(start_index)   #先加入开头index\n",
    "        for word in text:\n",
    "            text_index.append(w2i.get(word, unk_index))   #将当前词转成词典对应index，或不认识标注UNK的index\n",
    "        text_index.append(end_index)   #最后加个结尾index\n",
    "        #index\n",
    "        #1.2 tag标签\n",
    "        tag = tag.split()\n",
    "        tag_index = [tag2i.get(t,0) for t in tag]\n",
    "        tag_index = [0] + tag_index + [0]\n",
    "\n",
    "        #2. 填充或截至句子至标准长度\n",
    "        #2.1 text词汇&tag标签\n",
    "        if len(text_index)<MAX_LEN:    #句子短，补充pad_index到满够MAX_LEN\n",
    "            pad_len = MAX_LEN-len(text_index)\n",
    "            text_index = text_index + [pad_index]*pad_len\n",
    "            tag_index = tag_index + [0]*pad_len\n",
    "        elif len(text_index)>MAX_LEN:  #句子过长，截断\n",
    "            text_index = text_index[:MAX_LEN-1]\n",
    "            text_index.append(end_index)\n",
    "            tag_index = tag_index[:MAX_LEN-1]\n",
    "            tag_index.append(0)\n",
    "        masks.append([pad2mask(t) for t in text_index])\n",
    "        texts.append(text_index)\n",
    "        labels.append(tag_index)\n",
    "        \n",
    "    #把list类型的转成tensor类型，方便后期进行训练\n",
    "    texts = torch.LongTensor(texts)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    masks = torch.tensor(masks, dtype=torch.uint8)\n",
    "    #texts = torch.as_tensor(torch.from_numpy(np.array(texts)), dtype=torch.int32)\n",
    "    #labels = torch.as_tensor(torch.from_numpy(np.array(labels)), dtype=torch.int32)\n",
    "    return texts,labels,masks\n",
    "\n",
    "class MiDataset(Dataset):\n",
    "    def __init__(self, texts, labels, masks):\n",
    "        self.dataset = texts #torch.tensor(texts)\n",
    "        self.labels = labels    #torch.tensor(labels)\n",
    "        self.masks = masks\n",
    "        \n",
    "        self.nums = len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = {'texts': self.dataset[index],\n",
    "                'labels':self.labels[index],\n",
    "                'masks':self.masks[index]}\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.nums\n",
    "    \n",
    "unk_flag = '[UNK]'\n",
    "pad_flag = '[PAD]'\n",
    "start_flag = '[STA]'\n",
    "end_flag = '[END]' \n",
    "\n",
    "w2i = get_w2i()   #获得word_to_index词典\n",
    "tag2i = get_tag2index()\n",
    "\n",
    "#w2i\n",
    "unk_index = w2i.get(unk_flag, 101)\n",
    "pad_index = w2i.get(pad_flag, 1)\n",
    "start_index = w2i.get(start_flag, 102)    #开始\n",
    "end_index = w2i.get(end_flag, 103)   #中间截至（主要用在有上下句的情况下）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7528, 50])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7528, 50])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7528, 50])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.MiDataset at 0x288f307f7c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x288ec843cc8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将训练集的字符全部转成index，并改成MAX_LEN长度\n",
    "texts,labels,masks = text_tag_to_index(dataset)\n",
    "texts.shape\n",
    "labels.shape\n",
    "masks.shape\n",
    "\n",
    "#将数据集用MiDataset类包装\n",
    "train_dataset = MiDataset(texts,labels,masks)\n",
    "train_dataset\n",
    "\n",
    "#train_sampler = RandomSampler(train_dataset)    #将训练集打乱\n",
    "train_loader = DataLoader(dataset=train_dataset,   #按batch_size加载训练集\n",
    "                                batch_size=BATCH_SIZE, \n",
    "                                #sampler=train_sampler,\n",
    "                                num_workers=0,\n",
    "                                shuffle=False,\n",
    "                                drop_last=False)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 50])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:torch.Size([50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 102,  769, 6121,  123,  126, 2399, 4500, 6814, 8024, 1288, 2399, 1114,\n",
       "        1906, 2990, 7583, 8024, 1316, 4684, 2970, 6158, 7360, 1168,  123,  101,\n",
       "        8024, 1288, 2399, 3309, 7313, 1372,  101, 6814,  671, 3613,  676, 1283,\n",
       "        8024, 1071, 2124, 1059, 6956, 4696, 2141, 3867, 6589, 8024, 5018, 1063,\n",
       "         702,  103])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:torch.Size([50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 7, 8, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks:torch.Size([50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把第一个batch_data拿出来看看\n",
    "for step, batch_data in enumerate(train_loader):\n",
    "    print(\"step:\"+str(step))\n",
    "    batch_data['texts'].shape\n",
    "    print(\"texts:\"+str(batch_data['texts'][0].shape))\n",
    "    batch_data['texts'][0]\n",
    "    print(\"labels:\"+str(batch_data['labels'][0].shape))\n",
    "    batch_data['labels'][0]\n",
    "    print(\"masks:\"+str(batch_data['masks'][0].shape))\n",
    "    batch_data['masks'][0]\n",
    "    if step>=0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, pad_index,batch_size):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.pad_idx = pad_index\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #####中间层设置\n",
    "        #embedding层\n",
    "        self.word_embeds = nn.Embedding(vocab_size,embedding_dim,padding_idx=self.pad_idx)  #转词向量\n",
    "        #lstm层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, num_layers = 1, bidirectional = True)\n",
    "        #LSTM的输出对应tag空间（tag space）\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)  #输入是[batch_size, size]中的size，输出是[batch_size，output_size]的output_size\n",
    "        #CRF层\n",
    "        self.crf = CRF(self.tagset_size)   #默认batch_first=False\n",
    "        \n",
    "        #lstm层的隐藏节点设置\n",
    "        #hidden层的初始化似乎不能放在init里，得放在forward里：https://blog.csdn.net/qq_43473149/article/details/119490668\n",
    "        #self.hidden = (torch.randn(2,self.batch_size,self.hidden_dim//2),torch.randn(2,self.batch_size,self.hidden_dim//2))#self.init_hidden()\n",
    "    \n",
    "   \n",
    "    def forward(self, sentence, tags=None, mask=None):     #sentence=(batch,seq_len)   tags=(batch,seq_len)\n",
    "        #1. 从sentence到Embedding层\n",
    "        embeds = self.word_embeds(sentence).permute(1,0,2)#.view(MAX_LEN,len(sentence),-1)   #output=[seq_len, batch_size, embedding_size]\n",
    "        \n",
    "        #2. 从Embedding层到BiLSTM层\n",
    "        #隐藏层就是（h_0,c_0）   h_0的结构：(num_layers*num_directions,batch_size,hidden_size)=(2, 1, hidden_size=4//2=2)\n",
    "        self.hidden = (torch.randn(2,self.batch_size,self.hidden_dim//2),torch.randn(2,self.batch_size,self.hidden_dim//2))  #修改进来 shape=((2,1,2),(2,1,2))  \n",
    "        #input=(seq_length,batch_size,input_size)的张量  \n",
    "        #output=(seq_length,batch_size,num_directions*hidden_size)=(MAX_LEN,batch_size, 2)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden) \n",
    "        \n",
    "        #3. 从BiLSTM层到全连接层\n",
    "        #从lstm的输出转为tagset_size长度的向量组（即输出了每个tag的可能性）\n",
    "        #输出shape=(MAX_LEN, batch_size, len(tag_to_ix))\n",
    "        lstm_feats = self.hidden2tag(lstm_out)   \n",
    "        \n",
    "        #4. 全连接层到CRF层\n",
    "        if tags is not None: #训练用   #mask=attention_masks.byte()\n",
    "            if mask is not None:\n",
    "                loss = -1.*self.crf(emissions=lstm_feats,tags=tags.permute(1,0),mask=mask.permute(1,0),reduction='mean')   #outputs=(batch_size,)   输出log形式的likelihood\n",
    "            else:\n",
    "                loss = -1.*self.crf(emissions=lstm_feats,tags=tags.permute(1,0),reduction='mean')\n",
    "            return loss\n",
    "        else:   #测试用\n",
    "            if mask is not None:\n",
    "                prediction = self.crf.decode(emissions=lstm_feats,mask=mask.permute(1,0))   #mask=attention_masks.byte()\n",
    "            else:\n",
    "                prediction = self.crf.decode(emissions=lstm_feats)\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(21127, 120, padding_idx=1)\n",
       "  (lstm): LSTM(120, 6, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=12, out_features=9, bias=True)\n",
       "  (crf): CRF(num_tags=9)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建模型和优化器\n",
    "model = BiLSTM_CRF(len(w2i), tag2i, EMBEDDING_DIM, HIDDEN_DIM,pad_index,BATCH_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "#显示模型基本参数\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载已有的模型\n",
    "#batch_data['texts'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 18:27:35,691 - <ipython-input-17-bf898a0eb823>[line:11] - INFO: Epoch=0  step=0/941  loss=65.44607\n",
      "2021-12-13 18:27:44,371 - <ipython-input-17-bf898a0eb823>[line:11] - INFO: Epoch=0  step=100/941  loss=9.44542\n",
      "2021-12-13 18:27:53,368 - <ipython-input-17-bf898a0eb823>[line:11] - INFO: Epoch=0  step=200/941  loss=13.83408\n",
      "2021-12-13 18:28:02,481 - <ipython-input-17-bf898a0eb823>[line:11] - INFO: Epoch=0  step=300/941  loss=13.84288\n",
      "2021-12-13 18:28:11,600 - <ipython-input-17-bf898a0eb823>[line:11] - INFO: Epoch=0  step=400/941  loss=8.21426\n",
      "2021-12-13 18:28:21,178 - <ipython-input-17-bf898a0eb823>[line:11] - INFO: Epoch=0  step=500/941  loss=15.80962\n",
      "2021-12-13 18:28:30,590 - <ipython-input-17-bf898a0eb823>[line:11] - INFO: Epoch=0  step=600/941  loss=7.76563\n",
      "2021-12-13 18:28:40,234 - <ipython-input-17-bf898a0eb823>[line:11] - INFO: Epoch=0  step=700/941  loss=5.87144\n",
      "2021-12-13 18:28:49,840 - <ipython-input-17-bf898a0eb823>[line:11] - INFO: Epoch=0  step=800/941  loss=7.18524\n",
      "2021-12-13 18:28:59,220 - <ipython-input-17-bf898a0eb823>[line:11] - INFO: Epoch=0  step=900/941  loss=3.55480\n"
     ]
    }
   ],
   "source": [
    "samples_cnt = texts.shape[0]\n",
    "batch_cnt = math.ceil(samples_cnt/BATCH_SIZE)   #整除 向上取整\n",
    "for epoch in range(1):\n",
    "    for step, batch_data in enumerate(train_loader):\n",
    "        # 1. 清空梯度\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 2. 运行模型\n",
    "        loss = model(batch_data['texts'], batch_data['labels'],batch_data['masks']) \n",
    "        if step%100 ==0:\n",
    "            logger.info('Epoch=%d  step=%d/%d  loss=%.5f' % (epoch,step,batch_cnt,loss))\n",
    "        \n",
    "        # 3. 计算loss值，梯度并更新权重参数                                 \n",
    "        loss.backward()    #retain_graph=True)  #反向传播，计算当前梯度\n",
    "        optimizer.step()  #根据梯度更新网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存训练结果\n",
    "current_model_path = model_path+rq+\"_model.pkl\"\n",
    "torch.save(model,current_model_path)\n",
    "rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_masks:tensor数据，结构为(batch_size,MAX_LEN)\n",
    "#batch_labels: tensor数据，结构为(batch_size,MAX_LEN)\n",
    "#batch_prediction:list数据，结构为(batch_size,)   #每个数据长度不一（在model参数mask存在的情况下）\n",
    "def f1_score_evaluation(batch_masks,batch_labels,batch_prediction):\n",
    "    all_prediction = []\n",
    "    all_labels = []\n",
    "    for index in range(BATCH_SIZE):\n",
    "        #把没有mask掉的原始tag都集合到一起\n",
    "        length = sum(batch_masks[index].numpy()==1)\n",
    "        _label = batch_labels[index].numpy().tolist()[:length]\n",
    "        all_labels = all_labels+_label  \n",
    "        #把没有mask掉的预测tag都集合到一起\n",
    "        #_predict = y_pred[index][:length]\n",
    "        all_prediction = all_prediction+y_pred[index]\n",
    "        \n",
    "        assert len(_label)==len(y_pred[index])\n",
    "  \n",
    "        \n",
    "    assert len(all_prediction) == len(all_labels)\n",
    "    score = f1_score(all_prediction,all_labels,average='weighted')\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(21127, 120, padding_idx=1)\n",
       "  (lstm): LSTM(120, 6, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=12, out_features=9, bias=True)\n",
       "  (crf): CRF(num_tags=9)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score:0.9023501089074861\n"
     ]
    }
   ],
   "source": [
    "model.eval()   #不启用 BatchNormalization 和 Dropout，保证BN和dropout不发生变化\n",
    "with torch.no_grad():   #这部分的代码不用跟踪反向梯度更新\n",
    "    y_pred = model(sentence=batch_data['texts'],mask=batch_data['masks'])\n",
    "    score = f1_score_evaluation(batch_masks=batch_data['masks'],\n",
    "                        batch_labels=batch_data['labels'],\n",
    "                        batch_prediction=y_pred)\n",
    "    print(\"f1-score:\"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, sentence):\n",
    "        #从BiLSTM得到emission scores(也就是tagset_size长度的向量组)\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        \n",
    "        #给定lstm+全连接的特征值们，通过viterbi算法找到最优路径\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        \n",
    "        return score, tag_seq\n",
    "\n",
    "    def _viterbi_decode(self, feats):     #feats=(len(sentence)=11, batch_size=1, len(tag_to_ix)=5)=(11,1,5)\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
